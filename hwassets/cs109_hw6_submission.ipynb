{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 6\n",
    "# Reg-Logistic Regression, ROC, and Data Imputation\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine\n",
    "\n",
    "---\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- Do not include your name(s) in the notebook if you are submitting as a group. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your partner's name (if you submit separately):\n",
    "\n",
    "Enrollment Status (109A, 121A, 209A, or E109A):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn.apionly as sns\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Breast Cancer Detection\n",
    "\n",
    "In this homework, we will consider the problem of early breast cancer detection from X-ray images. Specifically, given a candidate region of interest (ROI) from an X-ray image of a patient's breast, the goal is to predict if the region corresponds to a malignant tumor (label 1) or is normal (label 0). The training and test data sets for this problem is provided in the file `hw6_dataset.csv`. Each row in these files corresponds to a ROI in a patient's X-ray, with columns 1-117 containing features computed using standard image processing algorithms. The last column contains the class label, and is based on a radiologist's opinion or a biopsy. This data was obtained from the KDD Cup 2008 challenge.\n",
    "\n",
    "The data set contain a total of 69,098 candidate ROIs, of which only 409 are malignant, while the remaining are all normal. \n",
    "\n",
    "*Note*: be careful of reading/treating column names and row names in this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Beyond Classification Accuracy\n",
    "\n",
    "\n",
    "0.  Split the data set into a training set and a testing set.  The training set should be 75% of the original data set, and the testing set 25%.  Use `np.random.seed(9001)`.\n",
    "\n",
    "1. Fit a logistic regression classifier to the training set and report the  accuracy of the classifier on the test set. You should use $L_2$ regularization in logistic regression, with the regularization parameter tuned using cross-validation. \n",
    "    1. How does the fitted model compare with a classifier that predicts 'normal' (label 0) on all patients? \n",
    "    2. Do you think the difference in the classification accuracies are large enough to declare logistic regression as a better classifier than the all 0's classifier? Why or why not?\n",
    "    \n",
    "For applications with imbalanced class labels, in this case when there are many more healthy subjects ($Y=0$) than those with cancer ($Y=1$), the classification accuracy may not be the best metric to evaluate a classifier's performance. As an alternative, we could analyze the confusion table for the classifier. \n",
    "\n",
    "<ol start=\"3\">\n",
    "<li> Compute the confusion table for both the fitted classifier and the classifier that predicts all 0's.</li>\n",
    "<li> Using the entries of the confusion table compute the *true positive rate* and the *true negative rate* for the two classifiers. Explain what these evaluation metrics mean for the specific task of cancer detection. Based on the observed metrics, comment on whether the fitted model is better than the all 0's classifier.</li>\n",
    "<li> What is the *false positive rate* of the fitted classifier, and how is it related to its true positive and true negative rate? Why is a classifier with high false positive rate undesirable for a cancer detection task?</li>\n",
    "</ol>\n",
    "*Hint:* You may use the `metrics.confusion_matrix` function to compute the confusion matrix for a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.1160</td>\n",
       "      <td>-0.1030</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.9800</td>\n",
       "      <td>-0.780</td>\n",
       "      <td>-0.474</td>\n",
       "      <td>-0.447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.34400</td>\n",
       "      <td>0.906</td>\n",
       "      <td>-1.1300</td>\n",
       "      <td>-0.552</td>\n",
       "      <td>0.553</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>0.2560</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.2230</td>\n",
       "      <td>-0.1730</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.284</td>\n",
       "      <td>-0.0522</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.427</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.00785</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>-0.0789</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.0723</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.212</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.2660</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>-1.190</td>\n",
       "      <td>-1.150</td>\n",
       "      <td>-1.8100</td>\n",
       "      <td>-1.560</td>\n",
       "      <td>-1.250</td>\n",
       "      <td>-1.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.816</td>\n",
       "      <td>1.570</td>\n",
       "      <td>0.39400</td>\n",
       "      <td>1.340</td>\n",
       "      <td>-1.1800</td>\n",
       "      <td>-2.700</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>-2.650</td>\n",
       "      <td>-0.0447</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.463</td>\n",
       "      <td>-1.0500</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.37100</td>\n",
       "      <td>0.859</td>\n",
       "      <td>-0.9930</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.0528</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.279</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.0973</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-1.3200</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-1.110</td>\n",
       "      <td>-1.090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.29500</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-1.1200</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>0.468</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.4350</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1       2       3      4      5       6      7      8      9    \\\n",
       "0 -0.144 -0.143 -0.1160 -0.1030  0.226  0.210 -0.9800 -0.780 -0.474 -0.447   \n",
       "1 -0.011  0.138 -0.2230 -0.1730  0.188  0.284 -0.0522 -0.256  0.129  0.427   \n",
       "2  0.212 -0.313  0.2660  0.2320 -1.190 -1.150 -1.8100 -1.560 -1.250 -1.200   \n",
       "3  0.215 -0.184  0.0274  0.0494  0.443  0.463 -1.0500 -0.941 -0.531 -0.394   \n",
       "4  0.279 -0.197  0.1270  0.0973 -0.213 -0.150 -1.3200 -0.994 -1.110 -1.090   \n",
       "\n",
       "  ...     108    109      110    111     112    113    114    115     116  117  \n",
       "0 ...   0.925  0.516  0.34400  0.906 -1.1300 -0.552  0.553 -0.417  0.2560  0.0  \n",
       "1 ...  -0.593  0.452  0.00785 -0.533 -0.0789  0.705  0.906  0.216 -0.0723  0.0  \n",
       "2 ...  -0.816  1.570  0.39400  1.340 -1.1800 -2.700 -0.926 -2.650 -0.0447  0.0  \n",
       "3 ...   0.634  0.111  0.37100  0.859 -0.9930 -0.492  0.363  0.326 -0.0528  0.0  \n",
       "4 ...  -0.640  0.485  0.29500  0.403 -1.1200 -0.343  0.468 -0.820  0.4350  0.0  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(9001)\n",
    "df = pd.read_csv('hw6_dataset.csv', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.14400</td>\n",
       "      <td>-0.1430</td>\n",
       "      <td>-0.1160</td>\n",
       "      <td>-0.1030</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.780</td>\n",
       "      <td>-0.474</td>\n",
       "      <td>-0.447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.906</td>\n",
       "      <td>-1.130</td>\n",
       "      <td>-0.552</td>\n",
       "      <td>0.553</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>0.2560</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.21500</td>\n",
       "      <td>-0.1840</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.463</td>\n",
       "      <td>-1.050</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.859</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.0528</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00922</td>\n",
       "      <td>-0.1380</td>\n",
       "      <td>0.1690</td>\n",
       "      <td>0.1540</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-1.690</td>\n",
       "      <td>-1.450</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.6990</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.481</td>\n",
       "      <td>-1.060</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>0.550</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.05690</td>\n",
       "      <td>0.1920</td>\n",
       "      <td>0.3020</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-0.607</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.0599</td>\n",
       "      <td>-1.070</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.817</td>\n",
       "      <td>-0.2830</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.09250</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>-0.2580</td>\n",
       "      <td>-0.2530</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.622</td>\n",
       "      <td>-1.790</td>\n",
       "      <td>-1.450</td>\n",
       "      <td>-1.170</td>\n",
       "      <td>-1.180</td>\n",
       "      <td>...</td>\n",
       "      <td>3.230</td>\n",
       "      <td>1.2000</td>\n",
       "      <td>0.270</td>\n",
       "      <td>2.220</td>\n",
       "      <td>-1.190</td>\n",
       "      <td>-1.350</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-1.530</td>\n",
       "      <td>0.1960</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3      4      5      6      7      8      9  \\\n",
       "0 -0.14400 -0.1430 -0.1160 -0.1030  0.226  0.210 -0.980 -0.780 -0.474 -0.447   \n",
       "3  0.21500 -0.1840  0.0274  0.0494  0.443  0.463 -1.050 -0.941 -0.531 -0.394   \n",
       "5  0.00922 -0.1380  0.1690  0.1540 -0.391 -0.397 -1.690 -1.450 -0.546 -0.527   \n",
       "6  0.05690  0.1920  0.3020  0.2720 -0.484 -0.473  0.348  0.256 -0.607 -0.355   \n",
       "9 -0.09250  0.0102 -0.2580 -0.2530  0.452  0.622 -1.790 -1.450 -1.170 -1.180   \n",
       "\n",
       "   ...      108     109    110    111    112    113    114    115     116  \\\n",
       "0  ...    0.925  0.5160  0.344  0.906 -1.130 -0.552  0.553 -0.417  0.2560   \n",
       "3  ...    0.634  0.1110  0.371  0.859 -0.993 -0.492  0.363  0.326 -0.0528   \n",
       "5  ...   -0.277  0.6990  0.371  0.481 -1.060 -0.526  0.550 -0.284  0.1550   \n",
       "6  ...    0.206 -0.0599 -1.070 -0.536  0.864  0.527  0.282  0.817 -0.2830   \n",
       "9  ...    3.230  1.2000  0.270  2.220 -1.190 -1.350  0.255 -1.530  0.1960   \n",
       "\n",
       "   label  \n",
       "0    0.0  \n",
       "3    0.0  \n",
       "5    0.0  \n",
       "6    0.0  \n",
       "9    0.0  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msk = np.random.rand(len(df)) < 0.75\n",
    "df_train = df[msk]\n",
    "df_test = df[~msk]\n",
    "df_train =df_train.rename(columns = {117:'label'})\n",
    "df_test = df_test.rename(columns = {117:'label'})\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Score: 0.995026040143\n",
      "All-Zeroes: 0.994206799696\n"
     ]
    }
   ],
   "source": [
    "#separating dependant variable from predictors\n",
    "x_train1 = df_train.loc[:, df_train.columns != 'label']\n",
    "y_train1 = df_train['label']\n",
    "x_test1 = df_test.loc[:, df_test.columns != 'label']\n",
    "y_test1 = df_test['label']\n",
    "\n",
    "#'all-zeroes' model\n",
    "all_zeroes = np.zeros_like(y_test1)\n",
    "\n",
    "log_regr = LogisticRegressionCV(random_state= 123, penalty='l2')\n",
    "log_regr.fit(x_train1, y_train1)\n",
    "print(\"Logistic Regression Score:\", log_regr.score(x_test1, y_test1))\n",
    "print(\"All-Zeroes:\", accuracy_score(y_test1, all_zeroes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    A. How does the fitted model compare with a classifier that predicts 'normal' (label 0) on all patients?\n",
    "    - According to their accuracy scores the Logistic Regression Classifier is only slightly better than the All-Zeroes classifier (difference is about  0.00082).\n",
    "    B. Do you think the difference in the classification accuracies are large enough to declare logistic regression as a better classifier than the all 0's classifier? Why or why not?\n",
    "    - The difference in accuracy score is very small, which means that I would not declare logistic regression as a better classifier, because it costs much more to perform computationally than the all zeroes classifier and its results are not significantly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Logistic Regression model:\n",
      " [[16984     6]\n",
      " [   79    20]]\n",
      "Confusion Matrix All Zeros:\n",
      " [[16990     0]\n",
      " [   99     0]]\n"
     ]
    }
   ],
   "source": [
    "con_matrix = metrics.confusion_matrix(y_test1, log_regr.predict(x_test1))\n",
    "con_matrix_zero = metrics.confusion_matrix(y_test1, all_zeroes)\n",
    "print(\"Confusion Matrix for Logistic Regression model:\\n\",con_matrix)\n",
    "print(\"Confusion Matrix All Zeros:\\n\",con_matrix_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rates(confusion_matrix):\n",
    "    rates = dict()\n",
    "    TN = confusion_matrix[0][0]\n",
    "    FN = confusion_matrix[0][1]\n",
    "    TP = confusion_matrix[1][1]\n",
    "    FP = confusion_matrix[1][0]\n",
    "    rates[\"True Positive Rate\"] = (TP/(TP+FN))\n",
    "    rates[\"False Positive Rate\"] = (FP/(FP+TN))\n",
    "    rates[\"True Negative Rate\"] = (TN/(FP+TN))\n",
    "    return rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Model: \n",
      "\n",
      "True Positive Rate 0.769230769231\n",
      "False Positive Rate 0.00462990095528\n",
      "True Negative Rate 0.995370099045\n",
      "\n",
      "All Zeroes: \n",
      "\n",
      "True Positive Rate nan\n",
      "False Positive Rate 0.00579320030429\n",
      "True Negative Rate 0.994206799696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Model: \\n\")\n",
    "for k, v in get_rates(con_matrix).items():\n",
    "    print(k, v)\n",
    "print(\"\\nAll Zeroes: \\n\")\n",
    "for k, v in get_rates(con_matrix_zero).items():\n",
    "    print(k, v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain what these evaluation metrics mean for the specific task of cancer detection. Based on the observed metrics, comment on whether the fitted model is better than the all 0's classifier.\n",
    "- The true positive rate describes the rate at which our model predicts that a patient has cancer when they actually do have cancer and the true negative rate describes the rate at which our model predicts that a patient is healthy when they are in fact healthy.  \n",
    "\n",
    "What is the false positive rate of the fitted classifier, and how is it related to its true positive and true negative rate? Why is a classifier with high false positive rate undesirable for a cancer detection task?\n",
    "- The false positive rate of the fitted classifier is 0.00463. This means that 0.46% of patients who are healthy are incorrectly classified as having cancer. The true negative rate would be equal to 1 minus the false positive rate. The true positive rate, however, cannot be directly derived from the false positive rate, because the equation for a true positive rate is TPR = TP/(TP+FN) and the false positive rate equation is FPR = FP/(FP + TN). A classifier with a high false positive rate would be undesirable, because it would incorrectly identify more people as having cancer and would not filter out as many people that were healthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: ROC Analysis\n",
    "\n",
    "Another powerful diagnostic tool for class-imbalanced classification tasks is the Receiver Operating Characteristic (ROC) curve. Notice that the default logistic regression classifier in `sklearn` classifies a data point by thresholding the predicted class probability $\\hat{P}(Y=1)$ at 0.5. By using a different threshold, we can adjust the trade-off between the true positive rate (TPR) and false positive rate (FPR) of the classifier. The ROC curve allows us to visualize this trade-off across all possible thresholds.\n",
    "\n",
    "\n",
    "1. Display the ROC curve for the fitted classifier on the *test set*. In the same plot, also display the ROC curve for the all 0's classifier. How do the two curves compare?\n",
    "\n",
    "2.  Compute the highest TPR that can be achieved by the classifier at each of the following FPR's, and the thresholds at which they are achieved. Based on your results, comment on how the threshold influences a classifier's FPR.\n",
    "    - FPR = 0\n",
    "    - FPR = 0.1\n",
    "    - FPR = 0.5\n",
    "    - FPR = 0.9\n",
    "- Suppose a clinician told you that diagnosing a cancer patient as normal is *twice* as critical an error as diagnosing a normal patient as having cancer. Based on this information, what threshold would you recommend the clinician to use? What is the TPR and FPR of the classifier at this threshold? \n",
    "\n",
    "- Compute the area under the ROC curve (AUC) for both the fitted classifier and the all 0's classifier. How does the difference in the AUCs of the two classifiers compare with the difference between their classification accuracies in Question 1, Part 2(A)? \n",
    "\n",
    "*Hint:* You may use the `metrics.roc_curve` function to compute the ROC curve for a classification model and the `metrics.roc_auc_score` function to compute the AUC for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def make_roc(name, clf, ytest, xtest, ax=None, labe=5, proba=True, skip=0):\n",
    "    initial=False\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "        initial=True\n",
    "    if proba:#for stuff like logistic regression\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n",
    "    else:#for stuff like SVM\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    if skip:\n",
    "        l=fpr.shape[0]\n",
    "        ax.plot(fpr[0:l:skip], tpr[0:l:skip], '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    else:\n",
    "        ax.plot(fpr, tpr, '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    label_kwargs = {}\n",
    "    label_kwargs['bbox'] = dict(\n",
    "        boxstyle='round,pad=0.3', alpha=0.2,\n",
    "    )\n",
    "    if labe!=None:\n",
    "        for k in range(0, fpr.shape[0],labe):\n",
    "            #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n",
    "            threshold = str(np.round(thresholds[k], 2))\n",
    "            ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n",
    "    if initial:\n",
    "        ax.plot([0, 1], [0, 1], 'k--')\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('ROC')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAAFrCAYAAADb81DWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x275a517bf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_context(\"notebook\")\n",
    "ax=make_roc(\"Logistic\", log_regr, y_test1, x_test1, labe=10, skip=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test1, log_regr.predict_proba(x_test1)[:,1])\n",
    "target_fprs = [0, 0.1, 0.5, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR: [  5.88581519e-05]\n",
      "TPR: [ 0.08080808]\n",
      "Threshold: [ 0.9305692]\n",
      "\n",
      "FPR: [ 0.17628016]\n",
      "TPR: [ 0.85858586]\n",
      "Threshold: [ 0.00256634]\n",
      "\n",
      "FPR: [ 0.57321954]\n",
      "TPR: [ 0.96969697]\n",
      "Threshold: [ 0.00012183]\n",
      "\n",
      "FPR: [ 0.17628016]\n",
      "TPR: [ 0.85858586]\n",
      "Threshold: [ 0.00256634]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tgt in target_fprs:\n",
    "    if tgt == 0.9:\n",
    "        idx = np.where((fpr < 0.15) & (fpr > 0.1))\n",
    "    elif tgt == 0:\n",
    "        idx = np.where(fpr == 0)\n",
    "    else:\n",
    "        idx = np.where((fpr > (tgt - 0.05)) & (fpr < (tgt + 0.05)))\n",
    "    tprs = tpr[idx]\n",
    "    max_tpr = np.max(tprs)\n",
    "    idz = np.where(tpr == max_tpr)\n",
    "    thresh = thresholds[idz]\n",
    "    min_thresh = np.min(thresh)\n",
    "    idq = np.where(thresholds == min_thresh)\n",
    "    print(\"FPR:\", fpr[idq])\n",
    "    print(\"TPR:\", tpr[idq])\n",
    "    print(\"Threshold: {}\\n\".format(thresholds[idq]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the highest TPR that can be achieved by the classifier at each of the following FPR's, and the thresholds at which they are achieved. Based on your results, comment on how the threshold influences a classifier's FPR.\n",
    "- The classifer's FPR increases as the threshold decreases. This is how we would expect the FPR to be influenced by the threshold, because as the threshold for a point being classified as positive is lowered more incorrect cassifications would slip through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new threshold that we would recommend is [ 0.00580326] \n",
      "\n",
      "At this threshold the FPR = [ 0.10453208] and the TPR = [ 0.84848485]\n"
     ]
    }
   ],
   "source": [
    "#create an array of (FNR*2 + FPR) and find the min\n",
    "sum_fnr_tpr = [(((1-tpr[i]) * 2) + fpr[i]) for i in range(len(fpr))]\n",
    "min_sum = np.min(sum_fnr_tpr)\n",
    "idw = np.where(sum_fnr_tpr == min_sum)\n",
    "print(\"The new threshold that we would recommend is {} \\n\".format(thresholds[idw]))\n",
    "print(\"At this threshold the FPR = {} and the TPR = {}\".format(fpr[idw], tpr[idw]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93463059078126776"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area under the curve for the all zeroes classifier is 0 because the false positive rate never changes and there is no area underneath a vertical line. The area under the curve of the logistic classifier model is 0.93, which is much greater than that of the all zeroes classifier and demonstrates to us that the logistic classifer is better than the all zeroes classifier than the accuracy score would indicate. This is because the accuracy score only gives the FPR and TPR at one threshold, rather than across many thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 3: Missing data\n",
    "\n",
    "In this problem you are given a different data set, `hw6_dataset_missing.csv`, that is  similar to the one you used above (same column definitions and same conditions), however this data set contains missing values. \n",
    "\n",
    "*Note*: be careful of reading/treating column names and row names in this data set as well, it *may* be different than the first data set.\n",
    "\n",
    "\n",
    "1. Remove all observations that contain and missing values, split the dataset into a 75-25 train-test split, and fit the regularized logistic regression as in Question 1 (use `LogisticRegressionCV` again to retune).  Report the overall classification rate and TPR in the test set.\n",
    "2. Restart with a fresh copy of the data in `hw6_dataset_missing.csv` and impute the missing data via mean imputation.  Split the data 75-25 and fit the regularized logistic regression model.  Report the overall classification rate and TPR in the test set.  \n",
    "3. Again restart with a fresh copy of the data in `hw6_dataset_missing.csv` and impute the missing data via a model-based imputation method. Once again split the data 75-25 and fit the regularized logistic regression model.  Report the overall classification rate and TPR in the test set.  \n",
    "4. Compare the results in the 3 previous parts of this problem.  Prepare a paragraph (5-6 sentences) discussing the results, the computational complexity of the methods, and conjecture and explain why you get the results that you see.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1290</td>\n",
       "      <td>-0.2160</td>\n",
       "      <td>0.2880</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.9550</td>\n",
       "      <td>-1.620</td>\n",
       "      <td>-1.470</td>\n",
       "      <td>-1.0100</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.1900</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>0.395</td>\n",
       "      <td>2.060</td>\n",
       "      <td>-1.180</td>\n",
       "      <td>-2.8500</td>\n",
       "      <td>-1.290</td>\n",
       "      <td>-2.100</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>0.3130</td>\n",
       "      <td>0.2810</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.2790</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.4320</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0181</td>\n",
       "      <td>0.2480</td>\n",
       "      <td>-0.869</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.6980</td>\n",
       "      <td>0.363</td>\n",
       "      <td>1.030</td>\n",
       "      <td>-0.2490</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.5790</td>\n",
       "      <td>0.5020</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>-0.2740</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0702</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.397</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.7380</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-0.2880</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.2170</td>\n",
       "      <td>-0.3570</td>\n",
       "      <td>-0.0539</td>\n",
       "      <td>-0.0688</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.6380</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0622</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-1.030</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>0.472</td>\n",
       "      <td>-0.390</td>\n",
       "      <td>0.3660</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.0846</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.4240</td>\n",
       "      <td>0.3520</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.0947</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.3020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7190</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>0.8530</td>\n",
       "      <td>0.953</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.1190</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       1       2       3       4      5       6      7      8  \\\n",
       "0           0  0.1290 -0.2160  0.2880  0.2370 -0.993 -0.9550 -1.620 -1.470   \n",
       "1           1  0.0989  0.1160  0.3130  0.2810 -0.188 -0.2790  0.173  0.445   \n",
       "2           2  0.0215  0.1590  0.5790  0.5020 -0.342 -0.2740 -0.172 -0.164   \n",
       "3           3 -0.2170 -0.3570 -0.0539 -0.0688  0.445  0.6380  0.436  0.351   \n",
       "4           4 -0.0846  0.0166  0.4240  0.3520 -0.259 -0.0947  0.119 -0.162   \n",
       "\n",
       "        9  ...      109     110    111    112    113     114    115    116  \\\n",
       "0 -1.0100  ...  -1.1900  1.1000  0.395  2.060 -1.180 -2.8500 -1.290 -2.100   \n",
       "1  0.4320  ...  -0.0181  0.2480 -0.869 -0.190  0.451  0.6980  0.363  1.030   \n",
       "2  0.2160  ...   0.0702  0.0200  0.397 -0.800  0.173  0.7380  0.465  0.440   \n",
       "3  0.0401  ...      NaN  0.0622  0.269 -0.217 -1.030  0.0276  0.472 -0.390   \n",
       "4  0.3020  ...   0.7190  0.3250 -0.286 -0.528 -0.704  0.8530  0.953 -0.116   \n",
       "\n",
       "      117  type  \n",
       "0  0.0121   0.0  \n",
       "1 -0.2490   0.0  \n",
       "2 -0.2880   0.0  \n",
       "3  0.3660   0.0  \n",
       "4 -0.1190   0.0  \n",
       "\n",
       "[5 rows x 119 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(9001)\n",
    "df_miss = pd.read_csv('hw6_dataset_missing.csv')\n",
    "df_miss1 = df_miss.copy()\n",
    "df_miss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24999, 119)\n",
      "0.6440257610304413\n"
     ]
    }
   ],
   "source": [
    "print(df_miss.shape)\n",
    "print(100*df_miss.type.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1436, 119)\n",
      "0.20891364902506965\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.0846</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.4240</td>\n",
       "      <td>0.3520</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.0947</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.1620</td>\n",
       "      <td>0.302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-0.2860</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>-0.7040</td>\n",
       "      <td>0.85300</td>\n",
       "      <td>0.953</td>\n",
       "      <td>-0.1160</td>\n",
       "      <td>-0.1190</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>-0.0713</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>-0.4380</td>\n",
       "      <td>-0.3920</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>0.0465</td>\n",
       "      <td>-1.080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.9420</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>0.0466</td>\n",
       "      <td>0.98400</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.2890</td>\n",
       "      <td>-0.2050</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.1640</td>\n",
       "      <td>0.4180</td>\n",
       "      <td>0.3540</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-0.4520</td>\n",
       "      <td>1.660</td>\n",
       "      <td>1.3400</td>\n",
       "      <td>0.400</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.010</td>\n",
       "      <td>-1.950</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>0.2810</td>\n",
       "      <td>-0.00374</td>\n",
       "      <td>-1.850</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>-0.4080</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>-0.1290</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>-0.2010</td>\n",
       "      <td>-0.1760</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.5530</td>\n",
       "      <td>-1.650</td>\n",
       "      <td>-1.4200</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>1.100</td>\n",
       "      <td>-0.0382</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-1.1900</td>\n",
       "      <td>0.42800</td>\n",
       "      <td>1.040</td>\n",
       "      <td>-3.2000</td>\n",
       "      <td>5.7300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>-0.0366</td>\n",
       "      <td>-0.0291</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-0.0993</td>\n",
       "      <td>0.201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.747</td>\n",
       "      <td>-0.6210</td>\n",
       "      <td>0.653</td>\n",
       "      <td>-0.6690</td>\n",
       "      <td>0.86000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>-0.0736</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0       1       2       3       4      5       6      7       8  \\\n",
       "4            4 -0.0846  0.0166  0.4240  0.3520 -0.259 -0.0947  0.119 -0.1620   \n",
       "54          54 -0.0713  0.1630 -0.4380 -0.3920  0.506  0.5750 -0.450  0.0465   \n",
       "55          55  0.0108  0.1640  0.4180  0.3540 -0.409 -0.4520  1.660  1.3400   \n",
       "81          81 -0.1290  0.0187 -0.2010 -0.1760  0.512  0.5530 -1.650 -1.4200   \n",
       "96          96 -0.0366 -0.0291  0.0428  0.0453  0.799  0.9050 -0.306 -0.0993   \n",
       "\n",
       "        9  ...     109    110     111    112     113      114    115     116  \\\n",
       "4   0.302  ...   0.719  0.325 -0.2860 -0.528 -0.7040  0.85300  0.953 -0.1160   \n",
       "54 -1.080  ...   0.158  0.193 -0.9420 -0.231  0.0466  0.98400  0.770  0.2890   \n",
       "55  0.400  ...  -1.010 -1.950  0.4750 -0.228  0.2810 -0.00374 -1.850  0.0613   \n",
       "81 -0.429  ...  -0.459  1.100 -0.0382  0.157 -1.1900  0.42800  1.040 -3.2000   \n",
       "96  0.201  ...   0.962  0.747 -0.6210  0.653 -0.6690  0.86000  0.998  0.5390   \n",
       "\n",
       "       117  type  \n",
       "4  -0.1190   0.0  \n",
       "54 -0.2050   0.0  \n",
       "55 -0.4080   0.0  \n",
       "81  5.7300   0.0  \n",
       "96 -0.0736   0.0  \n",
       "\n",
       "[5 rows x 119 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all observations that contain and missing values\n",
    "df_miss_all = df_miss.dropna(how = 'any')\n",
    "print(df_miss_all.shape)\n",
    "\n",
    "# checking how dropping observations affected the type outcome\n",
    "print(100*df_miss_all.type.mean())\n",
    "df_miss_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.084600</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.4240</td>\n",
       "      <td>0.3520</td>\n",
       "      <td>-0.2590</td>\n",
       "      <td>-0.0947</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-0.2860</td>\n",
       "      <td>-0.5280</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.9530</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.1190</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>-0.129000</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>-0.2010</td>\n",
       "      <td>-0.1760</td>\n",
       "      <td>0.5120</td>\n",
       "      <td>0.5530</td>\n",
       "      <td>-1.650</td>\n",
       "      <td>-1.420</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>1.100</td>\n",
       "      <td>-0.0382</td>\n",
       "      <td>0.1570</td>\n",
       "      <td>-1.190</td>\n",
       "      <td>0.428</td>\n",
       "      <td>1.0400</td>\n",
       "      <td>-3.200</td>\n",
       "      <td>5.7300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.0819</td>\n",
       "      <td>0.0794</td>\n",
       "      <td>-0.1130</td>\n",
       "      <td>-0.0859</td>\n",
       "      <td>-0.556</td>\n",
       "      <td>-0.564</td>\n",
       "      <td>0.458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>0.280</td>\n",
       "      <td>1.0200</td>\n",
       "      <td>-0.0604</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.9370</td>\n",
       "      <td>0.765</td>\n",
       "      <td>-0.0786</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>105</td>\n",
       "      <td>-0.369000</td>\n",
       "      <td>-0.0302</td>\n",
       "      <td>-0.1490</td>\n",
       "      <td>-0.1340</td>\n",
       "      <td>0.9610</td>\n",
       "      <td>1.0200</td>\n",
       "      <td>1.480</td>\n",
       "      <td>1.680</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>-1.860</td>\n",
       "      <td>0.2510</td>\n",
       "      <td>-0.9130</td>\n",
       "      <td>1.460</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-1.9200</td>\n",
       "      <td>0.382</td>\n",
       "      <td>-0.2820</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>144</td>\n",
       "      <td>-0.000257</td>\n",
       "      <td>-0.3280</td>\n",
       "      <td>-0.0651</td>\n",
       "      <td>-0.0632</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0372</td>\n",
       "      <td>-1.070</td>\n",
       "      <td>-1.020</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>...</td>\n",
       "      <td>1.740</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.3490</td>\n",
       "      <td>1.8900</td>\n",
       "      <td>-1.180</td>\n",
       "      <td>-1.340</td>\n",
       "      <td>-0.0111</td>\n",
       "      <td>-0.612</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0         1       2       3       4       5       6      7  \\\n",
       "4             4 -0.084600  0.0166  0.4240  0.3520 -0.2590 -0.0947  0.119   \n",
       "81           81 -0.129000  0.0187 -0.2010 -0.1760  0.5120  0.5530 -1.650   \n",
       "99           99  0.012100  0.1880  0.0819  0.0794 -0.1130 -0.0859 -0.556   \n",
       "105         105 -0.369000 -0.0302 -0.1490 -0.1340  0.9610  1.0200  1.480   \n",
       "144         144 -0.000257 -0.3280 -0.0651 -0.0632  0.0055  0.0372 -1.070   \n",
       "\n",
       "         8      9  ...     109    110     111     112    113    114     115  \\\n",
       "4   -0.162  0.302  ...   0.719  0.325 -0.2860 -0.5280 -0.704  0.853  0.9530   \n",
       "81  -1.420 -0.429  ...  -0.459  1.100 -0.0382  0.1570 -1.190  0.428  1.0400   \n",
       "99  -0.564  0.458  ...  -0.411  0.280  1.0200 -0.0604  0.158  0.608  0.9370   \n",
       "105  1.680 -0.208  ...  -0.405 -1.860  0.2510 -0.9130  1.460 -0.275 -1.9200   \n",
       "144 -1.020 -0.523  ...   1.740  0.991  0.3490  1.8900 -1.180 -1.340 -0.0111   \n",
       "\n",
       "       116     117  type  \n",
       "4   -0.116 -0.1190   0.0  \n",
       "81  -3.200  5.7300   0.0  \n",
       "99   0.765 -0.0786   0.0  \n",
       "105  0.382 -0.2820   0.0  \n",
       "144 -0.612  0.0623   0.0  \n",
       "\n",
       "[5 rows x 119 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 75 - 25 train-test split\n",
    "msk = np.random.rand(len(df_miss_all)) < 0.75\n",
    "df_train2 = df_miss_all[msk]\n",
    "df_test2 = df_miss_all[~msk]\n",
    "df_train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting training and testing by predictors and y-value\n",
    "x_train2 = df_train2.loc[:, df_train.columns != 'type']\n",
    "y_train2 = df_train2['type']\n",
    "x_test2 = df_test2.loc[:, df_test.columns != 'type']\n",
    "y_test2 = df_test2['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Score: 0.994382022472\n"
     ]
    }
   ],
   "source": [
    "# Classification rate\n",
    "log_miss_all = LogisticRegressionCV(penalty='l2')\n",
    "log_miss_all.fit(x_train2, y_train2)\n",
    "print(\"Logistic Regression Score:\", log_miss_all.score(x_test2, y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Logistic Regression model:\n",
      " [[354   0]\n",
      " [  2   0]]\n"
     ]
    }
   ],
   "source": [
    "# TPR (true prositive rate) in test set\n",
    "con_matrix1 = metrics.confusion_matrix(y_test2, log_miss_all.predict(x_test2))\n",
    "print(\"Confusion Matrix for Logistic Regression model:\\n\",con_matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Model: \n",
      "\n",
      "True Positive Rate nan\n",
      "False Positive Rate 0.00561797752809\n",
      "True Negative Rate 0.994382022472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# TPR\n",
    "print(\"Logistic Model: \\n\")\n",
    "for k, v in get_rates(con_matrix1).items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: mean imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting \n",
    "x_cols = df_miss1.loc[:, df_miss1.columns != 'type']\n",
    "x_cols_names = x_cols.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filling missing values with mean\n",
    "#df_miss1[x_cols_names]=df_miss1[x_cols_names].fillna(df_miss1.mean().iloc[0])\n",
    "#df_miss1\n",
    "df_miss1[df_miss1.columns]=df_miss1[df_miss1.columns].fillna(df_miss1.mean().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1290</td>\n",
       "      <td>-0.2160</td>\n",
       "      <td>0.2880</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.9550</td>\n",
       "      <td>-1.620</td>\n",
       "      <td>-1.470</td>\n",
       "      <td>-1.0100</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.1900</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>0.395</td>\n",
       "      <td>2.0600</td>\n",
       "      <td>-1.180</td>\n",
       "      <td>-2.8500</td>\n",
       "      <td>-1.290</td>\n",
       "      <td>-2.100</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.5790</td>\n",
       "      <td>0.5020</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>-0.2740</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0702</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.397</td>\n",
       "      <td>-0.8000</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.7380</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-0.2880</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.2170</td>\n",
       "      <td>-0.3570</td>\n",
       "      <td>-0.0539</td>\n",
       "      <td>-0.0688</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.6380</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>...</td>\n",
       "      <td>12499.0000</td>\n",
       "      <td>0.0622</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-0.2170</td>\n",
       "      <td>-1.030</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>0.472</td>\n",
       "      <td>-0.390</td>\n",
       "      <td>0.3660</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.0846</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.4240</td>\n",
       "      <td>0.3520</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.0947</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.3020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7190</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.5280</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>0.8530</td>\n",
       "      <td>0.953</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.1190</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0878</td>\n",
       "      <td>0.2180</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>0.8340</td>\n",
       "      <td>-1.410</td>\n",
       "      <td>-1.3100</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.7220</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4800</td>\n",
       "      <td>-0.9380</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.0344</td>\n",
       "      <td>1.060</td>\n",
       "      <td>0.2040</td>\n",
       "      <td>-0.716</td>\n",
       "      <td>0.595</td>\n",
       "      <td>-0.1710</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       1       2       3       4      5       6      7      8  \\\n",
       "0           0  0.1290 -0.2160  0.2880  0.2370 -0.993 -0.9550 -1.620 -1.470   \n",
       "2           2  0.0215  0.1590  0.5790  0.5020 -0.342 -0.2740 -0.172 -0.164   \n",
       "3           3 -0.2170 -0.3570 -0.0539 -0.0688  0.445  0.6380  0.436  0.351   \n",
       "4           4 -0.0846  0.0166  0.4240  0.3520 -0.259 -0.0947  0.119 -0.162   \n",
       "5           5  0.0878  0.2180  0.9710  0.8340 -1.410 -1.3100  0.564  0.707   \n",
       "\n",
       "        9  ...          109     110    111     112    113     114    115  \\\n",
       "0 -1.0100  ...      -1.1900  1.1000  0.395  2.0600 -1.180 -2.8500 -1.290   \n",
       "2  0.2160  ...       0.0702  0.0200  0.397 -0.8000  0.173  0.7380  0.465   \n",
       "3  0.0401  ...   12499.0000  0.0622  0.269 -0.2170 -1.030  0.0276  0.472   \n",
       "4  0.3020  ...       0.7190  0.3250 -0.286 -0.5280 -0.704  0.8530  0.953   \n",
       "5  0.7220  ...      -0.4800 -0.9380 -0.135 -0.0344  1.060  0.2040 -0.716   \n",
       "\n",
       "     116     117  type  \n",
       "0 -2.100  0.0121   0.0  \n",
       "2  0.440 -0.2880   0.0  \n",
       "3 -0.390  0.3660   0.0  \n",
       "4 -0.116 -0.1190   0.0  \n",
       "5  0.595 -0.1710   0.0  \n",
       "\n",
       "[5 rows x 119 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 75 - 25 train-test split\n",
    "msk = np.random.rand(len(df_miss1)) < 0.75\n",
    "df_train3 = df_miss1[msk]\n",
    "df_test3 = df_miss1[~msk]\n",
    "df_train3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting training and testing by predictors and y-value\n",
    "x_train3 = df_train3.loc[:, df_train.columns != 'type']\n",
    "y_train3 = df_train3['type']\n",
    "x_test3 = df_test3.loc[:, df_test.columns != 'type']\n",
    "y_test3 = df_test3['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Score: 0.994947025265\n"
     ]
    }
   ],
   "source": [
    "# Classification rate\n",
    "log_miss1 = LogisticRegressionCV(penalty='l2')\n",
    "log_miss1.fit(x_train3, y_train3)\n",
    "print(\"Logistic Regression Score:\", log_miss1.score(x_test3, y_test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Logistic Regression model:\n",
      " [[6104    0]\n",
      " [  31    0]]\n"
     ]
    }
   ],
   "source": [
    "# TPR (true prositive rate) in test set\n",
    "con_matrix2 = metrics.confusion_matrix(y_test3, log_miss1.predict(x_test3))\n",
    "print(\"Confusion Matrix for Logistic Regression model:\\n\",con_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Model: \n",
      "\n",
      "True Positive Rate nan\n",
      "False Positive Rate 0.00505297473513\n",
      "True Negative Rate 0.994947025265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# TPR\n",
    "print(\"Logistic Model: \\n\")\n",
    "for k, v in get_rates(con_matrix2).items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3: model-based imputatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# making a copy of the dataset\n",
    "df_miss2 = df_miss.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using dataset without NAs to build model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# columns with all the data\n",
    "df1 = df_miss_all.iloc[:,1:]\n",
    "# all columns, with even missing values\n",
    "df_miss1 = df_miss2.iloc[:,1:]\n",
    "x_temp = df1.loc[:, df1.columns != 'type']\n",
    "y_temp = df1['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# columns that have missing values\n",
    "mcols = df_miss1.columns[df_miss1.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#for mcol in y_temp[mcols]: \n",
    "regress = LinearRegression()\n",
    "regress.fit(x_temp,y_temp)\n",
    "y_hat = regress.predict(x_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0\n",
       "5          0\n",
       "6          0\n",
       "7          0\n",
       "8          0\n",
       "9          0\n",
       "10         0\n",
       "11         0\n",
       "12         0\n",
       "13         0\n",
       "14         0\n",
       "15         0\n",
       "16         0\n",
       "17         0\n",
       "18         0\n",
       "19         0\n",
       "20         0\n",
       "21         0\n",
       "22         0\n",
       "23         0\n",
       "24         0\n",
       "25         0\n",
       "26         0\n",
       "27         0\n",
       "28         0\n",
       "29         0\n",
       "30         0\n",
       "        ... \n",
       "89         0\n",
       "90         0\n",
       "91         0\n",
       "92         0\n",
       "93      3894\n",
       "94      3903\n",
       "95      3897\n",
       "96      3913\n",
       "97      3899\n",
       "98      3903\n",
       "99      3911\n",
       "100     3907\n",
       "101     3889\n",
       "102     3953\n",
       "103     3911\n",
       "104     3928\n",
       "105     3891\n",
       "106     3892\n",
       "107     3928\n",
       "108     3909\n",
       "109     3914\n",
       "110        0\n",
       "111        0\n",
       "112        0\n",
       "113        0\n",
       "114        0\n",
       "115        0\n",
       "116        0\n",
       "117        0\n",
       "type       0\n",
       "Length: 118, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_miss1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a list of columns that have NaN values\n",
    "nan_cols = df_miss1.columns[df_miss1.isnull().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop all columns with null values, this dataset will be what we use to fit linear regressions and estimate values to impute\n",
    "df_no_null_cols = df_miss1.dropna(axis = 1, how='any')\n",
    "df_no_null_colsx = df_no_null_cols.loc[:, df_no_null_cols.columns != 'type']\n",
    "#impute_model is a linear regression model that we will fit to impute missing values\n",
    "impute_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_imputed is basically an accumulator that will end up being our full imputed dataset\n",
    "df_imputed = df_miss1.copy()\n",
    "df_iterator = df_miss1.iterrows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1290</td>\n",
       "      <td>-0.2160</td>\n",
       "      <td>0.2880</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.9550</td>\n",
       "      <td>-1.620</td>\n",
       "      <td>-1.470</td>\n",
       "      <td>-1.0100</td>\n",
       "      <td>-1.0100</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.190000</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>0.395</td>\n",
       "      <td>2.060</td>\n",
       "      <td>-1.180</td>\n",
       "      <td>-2.8500</td>\n",
       "      <td>-1.290</td>\n",
       "      <td>-2.100</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>0.3130</td>\n",
       "      <td>0.2810</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.2790</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.4320</td>\n",
       "      <td>0.9440</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018100</td>\n",
       "      <td>0.2480</td>\n",
       "      <td>-0.869</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.6980</td>\n",
       "      <td>0.363</td>\n",
       "      <td>1.030</td>\n",
       "      <td>-0.2490</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.5790</td>\n",
       "      <td>0.5020</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>-0.2740</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>0.0709</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.397</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.7380</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-0.2880</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.2170</td>\n",
       "      <td>-0.3570</td>\n",
       "      <td>-0.0539</td>\n",
       "      <td>-0.0688</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.6380</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>-0.1140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006779</td>\n",
       "      <td>0.0622</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-1.030</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>0.472</td>\n",
       "      <td>-0.390</td>\n",
       "      <td>0.3660</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.0846</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.4240</td>\n",
       "      <td>0.3520</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.0947</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.3020</td>\n",
       "      <td>-0.1700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719000</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>0.8530</td>\n",
       "      <td>0.953</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.1190</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1       2       3       4      5       6      7      8       9  \\\n",
       "0  0.1290 -0.2160  0.2880  0.2370 -0.993 -0.9550 -1.620 -1.470 -1.0100   \n",
       "1  0.0989  0.1160  0.3130  0.2810 -0.188 -0.2790  0.173  0.445  0.4320   \n",
       "2  0.0215  0.1590  0.5790  0.5020 -0.342 -0.2740 -0.172 -0.164  0.2160   \n",
       "3 -0.2170 -0.3570 -0.0539 -0.0688  0.445  0.6380  0.436  0.351  0.0401   \n",
       "4 -0.0846  0.0166  0.4240  0.3520 -0.259 -0.0947  0.119 -0.162  0.3020   \n",
       "\n",
       "       10  ...        109     110    111    112    113     114    115    116  \\\n",
       "0 -1.0100  ...  -1.190000  1.1000  0.395  2.060 -1.180 -2.8500 -1.290 -2.100   \n",
       "1  0.9440  ...  -0.018100  0.2480 -0.869 -0.190  0.451  0.6980  0.363  1.030   \n",
       "2  0.0709  ...   0.070200  0.0200  0.397 -0.800  0.173  0.7380  0.465  0.440   \n",
       "3 -0.1140  ...   0.006779  0.0622  0.269 -0.217 -1.030  0.0276  0.472 -0.390   \n",
       "4 -0.1700  ...   0.719000  0.3250 -0.286 -0.528 -0.704  0.8530  0.953 -0.116   \n",
       "\n",
       "      117  type  \n",
       "0  0.0121   0.0  \n",
       "1 -0.2490   0.0  \n",
       "2 -0.2880   0.0  \n",
       "3  0.3660   0.0  \n",
       "4 -0.1190   0.0  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#iterate through the list of columns that contain NaN values\n",
    "for i in nan_cols:\n",
    "    #this will be an array of rows that have null values\n",
    "    null_rows = [p for p in range(len(df_miss1)) if df_miss1.iloc[p].isnull()[i]]\n",
    "    \n",
    "    x_predict = []\n",
    "    impute_indices = []\n",
    "    x_fit = []\n",
    "    y_fit = []\n",
    "    \n",
    "    for h in range(len(df_no_null_cols)):\n",
    "        if h in null_rows:\n",
    "            x_predict.append(df_no_null_colsx.iloc[h].tolist())\n",
    "            impute_indices.append(h)\n",
    "        else:\n",
    "            x_fit.append(df_no_null_colsx.loc[h].tolist())\n",
    "            y_fit.append(df_no_null_cols.iloc[h]['type'].tolist())\n",
    "    \n",
    "    x_fit1 = np.asarray(x_fit)\n",
    "    y_fit1 = np.asarray(y_fit)\n",
    "    x_predict1 = np.asarray(x_predict)\n",
    "    impute_model.fit(x_fit1, y_fit1)\n",
    "    to_impute = impute_model.predict(x_predict1)\n",
    "    \n",
    "    for z in range(len(df_imputed)):\n",
    "        if z in impute_indices:\n",
    "            df_imputed[i][z] = to_impute[impute_indices.index(z)]\n",
    "    print(\"1\")\n",
    "    \n",
    "df_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75 - 25 train-test split\n",
    "msk = np.random.rand(len(df_imputed)) < 0.75\n",
    "df_train4 = df_imputed[msk]\n",
    "df_test4 = df_imputed[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train4 = df_train4.loc[:, df_train.columns != 'type']\n",
    "y_train4 = df_train4['type']\n",
    "x_test4 = df_test4.loc[:, df_test.columns != 'type']\n",
    "y_test4 = df_test4['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Score: 0.999837846603\n"
     ]
    }
   ],
   "source": [
    "# Classification rate\n",
    "log_miss1 = LogisticRegressionCV(penalty='l2')\n",
    "log_miss1.fit(x_train4, y_train4)\n",
    "print(\"Logistic Regression Score:\", log_miss1.score(x_test4, y_test4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Logistic Regression model:\n",
      " [[6124    1]\n",
      " [   0   42]]\n"
     ]
    }
   ],
   "source": [
    "# TPR (true prositive rate) in test set\n",
    "con_matrix3 = metrics.confusion_matrix(y_test4, log_miss1.predict(x_test4))\n",
    "print(\"Confusion Matrix for Logistic Regression model:\\n\",con_matrix3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Model: \n",
      "\n",
      "True Positive Rate 0.976744186047\n",
      "False Positive Rate 0.0\n",
      "True Negative Rate 1.0\n"
     ]
    }
   ],
   "source": [
    "# TPR\n",
    "print(\"Logistic Model: \\n\")\n",
    "for k, v in get_rates(con_matrix3).items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APCOMP209a - Homework Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "This problem walks you through the derivation of the **likelihood equations** for a generalized linear model (GLM). Suppose that the random component of the GLM is in the univariate natural exponential family, so that\n",
    "$$f(y_i|\\theta_i) = h(y_i) e^{y_i\\theta_i - b(\\theta_i)}$$\n",
    "Define the individual log-likelihood for each observation $i$ as\n",
    "$$l_i(\\theta_i) \\equiv \\log f(y_i|\\theta_i)$$\n",
    "with linear predictor\n",
    "$$\\eta_i = x_i^T\\beta = g(\\mu_i)$$\n",
    "for some link function $g$ and where $\\mu_i=E(Y_i)$.\n",
    "\n",
    "1. Use the above expressions to write a simplified expression for the log-likelihood $l(\\theta)$ for the entire dataset, $y_1, \\dots, y_n$.\n",
    "\n",
    "2. Use the chain rule to express $\\frac{\\partial l_i}{\\partial \\beta_j}$ in terms of the derivatives of $l_i, \\theta_i, \\mu_i$, and $\\eta_i$. (*Hint*: Think carefully about which variables are related to which, and in what way. For example, for which of the above variables do you know the derivative with respect to $\\beta_j$?)\n",
    "\n",
    "3. Compute the derivatives for $\\frac{\\partial l_i}{\\partial \\theta_i}$ and $\\frac{\\partial \\eta_i}{\\partial \\beta_j}$.\n",
    "\n",
    "4. Express $\\mu_i$ in terms of $\\theta_i$, and use this relationship to compute $\\frac{\\partial \\theta_i}{\\partial \\mu_i}$. (\\emph{Hint}: Recall the cumulant function of a natural exponential family, and assume that you can write $\\partial f/\\partial g = (\\partial g / \\partial f)^{-1}$.)\n",
    "\n",
    "5. Express $\\eta_i$ in terms of $\\mu_i$. Using the same hint as the above, compute $\\frac{\\partial \\mu_i}{\\partial \\eta_i}$.\n",
    "\n",
    "6. Put all of the above parts together to write an expression for $\\frac{\\partial l}{\\partial \\beta_j}$. Use matrix notation to write this expression as\n",
    "$$\\nabla_{\\beta} l(\\beta) = XDV^{-1}(Y - \\mu) = 0$$\n",
    "That is, compute the matrices $D$ and $V$ such that this equation holds.\n",
    "\n",
    "7. If we use the canonical link function, how do your answers to part (6) simplify?\n",
    "\n",
    "8. Finally, compute the above likelihood equations in the case of logistic regression, and show that this is equivalent to the solution given in lecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

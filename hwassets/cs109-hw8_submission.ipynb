{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 8\n",
    "# Ensemble methods\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as random\n",
    "import itertools as itertools\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Boson Discovery\n",
    "\n",
    "The discovery of the Higgs boson in July 2012 marked a fundamental breakthrough in particle physics. The Higgs boson particle was discovered through experiments at the Large Hadron Collider at CERN, by colliding beams of protons at high energy. A key challenge in analyzing the results of these experiments is to differentiate between a collision that produces Higgs bosons and collisions thats produce only background noise. We shall explore the use of ensemble methods for this classification task.\n",
    "\n",
    "You are provided with data from Monte-Carlo simulations of collisions of particles in a particle collider experiment. The training set is available in `Higgs_train.csv` and the test set is in `Higgs_test.csv`. Each row in these files corresponds to a particle colision described by 28 features (columns 1-28), of which the first 21 features are kinematic properties measured by the particle detectors in the accelerator, and the remaining features are derived by physicists from the the first 21 features. The class label is provided in the last column, with a label of 1 indicating that the collision produces Higgs bosons (signal), and a label of 0 indicating that the collision produces other particles (background). \n",
    "\n",
    "The data set provided to you is a small subset of the HIGGS data set in the UCI machine learning repository. The following paper contains further details about the data set and the predictors used: <a href = \"https://www.nature.com/articles/ncomms5308\">Baldi et al., Nature Communications 5, 2014</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton pT</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>jet 2 pt</th>\n",
       "      <th>...</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.376816</td>\n",
       "      <td>-1.583727</td>\n",
       "      <td>-1.707552</td>\n",
       "      <td>0.990897</td>\n",
       "      <td>0.114397</td>\n",
       "      <td>1.253553</td>\n",
       "      <td>0.619859</td>\n",
       "      <td>-1.479572</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>0.753658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522449</td>\n",
       "      <td>1.318622</td>\n",
       "      <td>0.982398</td>\n",
       "      <td>1.359610</td>\n",
       "      <td>0.964809</td>\n",
       "      <td>1.309991</td>\n",
       "      <td>1.083203</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.707330</td>\n",
       "      <td>0.087603</td>\n",
       "      <td>-0.399742</td>\n",
       "      <td>0.918742</td>\n",
       "      <td>-1.229936</td>\n",
       "      <td>1.172847</td>\n",
       "      <td>-0.552574</td>\n",
       "      <td>0.886053</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>1.298317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.439696</td>\n",
       "      <td>0.828885</td>\n",
       "      <td>0.992241</td>\n",
       "      <td>1.157820</td>\n",
       "      <td>2.215780</td>\n",
       "      <td>1.189586</td>\n",
       "      <td>0.937976</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.617290</td>\n",
       "      <td>0.265839</td>\n",
       "      <td>-1.345227</td>\n",
       "      <td>1.154581</td>\n",
       "      <td>1.036646</td>\n",
       "      <td>0.954822</td>\n",
       "      <td>0.377252</td>\n",
       "      <td>-0.147960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.063507</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.542413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.024506</td>\n",
       "      <td>1.026255</td>\n",
       "      <td>0.986289</td>\n",
       "      <td>0.927720</td>\n",
       "      <td>1.371080</td>\n",
       "      <td>0.981672</td>\n",
       "      <td>0.917436</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.850992</td>\n",
       "      <td>-0.380876</td>\n",
       "      <td>-0.071264</td>\n",
       "      <td>1.468704</td>\n",
       "      <td>-0.795133</td>\n",
       "      <td>0.691818</td>\n",
       "      <td>0.883260</td>\n",
       "      <td>0.496881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.616349</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.520171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.197755</td>\n",
       "      <td>1.100534</td>\n",
       "      <td>0.987262</td>\n",
       "      <td>1.353453</td>\n",
       "      <td>1.455383</td>\n",
       "      <td>0.994682</td>\n",
       "      <td>0.953553</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.767540</td>\n",
       "      <td>-0.691572</td>\n",
       "      <td>-0.040191</td>\n",
       "      <td>0.614843</td>\n",
       "      <td>0.143765</td>\n",
       "      <td>0.748614</td>\n",
       "      <td>0.397057</td>\n",
       "      <td>-0.873640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.147862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502034</td>\n",
       "      <td>1.550981</td>\n",
       "      <td>0.921948</td>\n",
       "      <td>0.864080</td>\n",
       "      <td>0.982839</td>\n",
       "      <td>1.373222</td>\n",
       "      <td>0.601492</td>\n",
       "      <td>0.918621</td>\n",
       "      <td>0.957063</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lepton pT   lepton eta   lepton phi   missing energy magnitude  \\\n",
       "0   0.376816    -1.583727    -1.707552                   0.990897   \n",
       "1   0.707330     0.087603    -0.399742                   0.918742   \n",
       "2   0.617290     0.265839    -1.345227                   1.154581   \n",
       "3   0.850992    -0.380876    -0.071264                   1.468704   \n",
       "4   0.767540    -0.691572    -0.040191                   0.614843   \n",
       "\n",
       "    missing energy phi   jet 1 pt   jet 1 eta   jet 1 phi   jet 1 b-tag  \\\n",
       "0             0.114397   1.253553    0.619859   -1.479572      2.173076   \n",
       "1            -1.229936   1.172847   -0.552574    0.886053      2.173076   \n",
       "2             1.036646   0.954822    0.377252   -0.147960      0.000000   \n",
       "3            -0.795133   0.691818    0.883260    0.496881      0.000000   \n",
       "4             0.143765   0.748614    0.397057   -0.873640      0.000000   \n",
       "\n",
       "    jet 2 pt   ...     jet 4 phi   jet 4 b-tag      m_jj     m_jjj      m_lv  \\\n",
       "0   0.753658   ...      0.397156      0.000000  0.522449  1.318622  0.982398   \n",
       "1   1.298317   ...      0.236231      0.000000  0.439696  0.828885  0.992241   \n",
       "2   1.063507   ...     -0.542413      0.000000  1.024506  1.026255  0.986289   \n",
       "3   1.616349   ...     -1.520171      0.000000  1.197755  1.100534  0.987262   \n",
       "4   1.147862   ...      0.502034      1.550981  0.921948  0.864080  0.982839   \n",
       "\n",
       "      m_jlv      m_bb     m_wbb    m_wwbb   class  \n",
       "0  1.359610  0.964809  1.309991  1.083203     1.0  \n",
       "1  1.157820  2.215780  1.189586  0.937976     1.0  \n",
       "2  0.927720  1.371080  0.981672  0.917436     1.0  \n",
       "3  1.353453  1.455383  0.994682  0.953553     1.0  \n",
       "4  1.373222  0.601492  0.918621  0.957063     0.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('Higgs_train.csv')\n",
    "df_test = pd.read_csv('Higgs_test.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into x and y training and testing\n",
    "x_train = df_train.loc[:, df_train.columns != ' class']\n",
    "y_train = df_train[\" class\"]\n",
    "x_test = df_test.loc[:, df_test.columns != ' class']\n",
    "y_test = df_test[' class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (2pt): Single Decision Tree\n",
    "We start by building a basic model which we will use as our base model for comparison. \n",
    "\n",
    "1. Fit a decision tree model to the training set and report the classification accuracy of the model on the test set. Use 5-fold cross-validation to choose the (maximum) depth for the tree. You will use the max_depth you find here throughout the homework. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10': 0.60780235355533108,\n",
       " '2': 0.62200179832124691,\n",
       " '3': 0.62199795985540496,\n",
       " '4': 0.63620340222419902,\n",
       " '5': 0.63180308094460802,\n",
       " '6': 0.62540651752307641,\n",
       " '7': 0.62060147800127252,\n",
       " '8': 0.61520051665750231,\n",
       " '9': 0.6146038751231907}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decision tree with CV to determine best depth (found to be 4)\n",
    "depths = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "tree_scores = dict()\n",
    "for i, d in enumerate(depths):\n",
    "    tree_model =  DecisionTreeClassifier(max_depth = d, random_state = 1)\n",
    "    scores = cross_val_score(tree_model, x_train, y_train)\n",
    "    tree_scores[str(d)] = np.mean(scores)\n",
    "tree_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64959999999999996"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_model =  DecisionTreeClassifier(max_depth = 4, random_state = 1)\n",
    "tree_model.fit(x_train, y_train)\n",
    "tree_model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (15pt): Dropout-based Approach\n",
    "We start with a simple method inspired from the idea of 'dropout' in machine learning, where we fit multiple decision trees on random subsets of predictors, and combine them through a majority vote. The procedure is described below.\n",
    "\n",
    "- For each predictor in the training sample, set the predictor values to 0 with probability $p$  (i.e. drop the predictor by setting it to 0). Repeat this for $B$ trials to create $B$ separate training sets.\n",
    "\n",
    "\n",
    "- Fit decision tree models $\\hat{h}^1(x), \\ldots, \\hat{h}^B(x) \\in \\{0,1\\}$ to the $B$ training sets. \n",
    "\n",
    "- Combine the decision tree models into a single classifier by taking a majority vote:\n",
    "$$\n",
    "\\hat{H}_{maj}(x) \\,=\\, majority\\Big(\\hat{h}^1(x), \\ldots, \\hat{h}^B(x)\\Big).\n",
    "$$\n",
    "\n",
    "\n",
    "We shall refer to the combined classifier as an ** *ensemble classifier* **. Implement the described dropout approach, and answer the following questions:\n",
    "1. Apply the dropout procedure with $p = 0.5$ for different number of trees (say $2, 4, 8, 16, \\ldots, 256$), and evaluate the training and test accuracy of the combined classifier. Does an increase in the number of trees improve the training and test performance? Explain your observations in terms of the bias-variance trade-off for the classifier.\n",
    "- Fix the number of trees to 64 and apply the dropout procedure with different dropout rates $p = 0.1, 0.3, 0.5, 0.7, 0.9$. Based on your results, explain how the dropout rate influences the bias and variance of the combined classifier.\n",
    "- Apply 5-fold cross-validation to choose the optimal combination of the dropout rate and number of trees. How does the test performance of an ensemble of trees fitted with the optimal dropout rate and number of trees compare with the single decision tree model in Question 1?\n",
    "[hint: Training with large number of trees can take long time. You may need to restrict the max number of trees.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flip(probability):\n",
    "    return 0 if random.random() < probability else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function creates b new sets with probability p to determine the new columns\n",
    "def create_sets(train, test, p, b):\n",
    "    set_list = []\n",
    "    for i in range(b):\n",
    "        new_columns = []\n",
    "        while len(new_columns) == 0:\n",
    "            for c in train.columns:\n",
    "                if flip(p) == 1:\n",
    "                    new_columns.append(c)\n",
    "        set_list.append((train[new_columns], test[new_columns]))\n",
    "    return set_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_array(train_list, train_y):\n",
    "    m_array = []\n",
    "    count = 0\n",
    "    for train in train_list:\n",
    "        tree_model = DecisionTreeClassifier(max_depth = 4, random_state = 1)\n",
    "        tree_model.fit(train, train_y)\n",
    "        m_array.append(deepcopy(tree_model))\n",
    "    return m_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensemble_classifier(m_array, x_list):\n",
    "    ps = []\n",
    "    predictions = []\n",
    "    for m, test in zip(m_array, x_list):\n",
    "        p1 = m.predict(test)\n",
    "        ps.append(p1)\n",
    "    for i in range(len(ps[0])):\n",
    "        voting = []\n",
    "        for p in ps:\n",
    "            voting.append(p[i])\n",
    "        if np.mean(voting) >= 0.5:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'128': (0.68379999999999996, 0.66139999999999999),\n",
       " '16': (0.67500000000000004, 0.65680000000000005),\n",
       " '2': (0.66279999999999994, 0.63959999999999995),\n",
       " '256': (0.67920000000000003, 0.66039999999999999),\n",
       " '32': (0.68120000000000003, 0.65559999999999996),\n",
       " '4': (0.64600000000000002, 0.624),\n",
       " '64': (0.67700000000000005, 0.65700000000000003),\n",
       " '8': (0.66720000000000002, 0.6522)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_counts = [2**x for x in range(1, 9)]\n",
    "c_scores = dict()\n",
    "for count in tree_counts:\n",
    "    set_list = create_sets(x_train, x_test, 0.5, count)\n",
    "    train_list = [x[0] for x in set_list]\n",
    "    test_list = [x[1] for x in set_list]\n",
    "    m = model_array(train_list, y_train)\n",
    "    train_predictions = ensemble_classifier(m, train_list)\n",
    "    test_predictions = ensemble_classifier(m, test_list)\n",
    "    c_scores[str(count)] = (metrics.accuracy_score(y_train, train_predictions), metrics.accuracy_score(y_test, test_predictions)) \n",
    "c_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does an increase in the number of trees improve the training and test performance? Explain your observations in terms of the bias-variance trade-off for the classifier.\n",
    "- An increase in the number of trees does not necessarily improve the training and test performance. This is shown clearly in the dictionary above, when we use 256 trees instead of 128 and both the training and test scores decrease. This turning point where the scores cease increasing shows where the bias-variance trade-off happens. As we average more trees the variance of our model decreases and at a certain point we reach hit a plateau of training and testing scores, where an increase in the number of trees does not have a perceptible effect on the scores and our model is limited by its fundamental bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.1', (0.66020000000000001, 0.64959999999999996)),\n",
       " ('0.3', (0.66679999999999995, 0.65180000000000005)),\n",
       " ('0.5', (0.67859999999999998, 0.65700000000000003)),\n",
       " ('0.7', (0.69520000000000004, 0.66479999999999995)),\n",
       " ('0.9', (0.54620000000000002, 0.54139999999999999))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "p_scores = dict()\n",
    "for prob in probabilities:\n",
    "    set_list = create_sets(x_train, x_test, prob, 64)\n",
    "    train_list = [x[0] for x in set_list]\n",
    "    test_list = [x[1] for x in set_list]\n",
    "    m = model_array(train_list, y_train)\n",
    "    train_predictions = ensemble_classifier(m, train_list)\n",
    "    test_predictions = ensemble_classifier(m, test_list)\n",
    "    p_scores[str(prob)] = (metrics.accuracy_score(y_train, train_predictions), metrics.accuracy_score(y_test, test_predictions)) \n",
    "p_scores = sorted(p_scores.items())\n",
    "p_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your results, explain how the dropout rate influences the bias and variance of the combined classifier.\n",
    "- Our results show that the accuracy of our ensemble classifier increases as the dropout rate increases up to a point. Similar to the pattern we observed when we were looking at the number of trees, the scores begin to decrease once the dropout rate gets as high as 0.9. This happens because when the dropout rate increases, the bias of our ensemble model decreases due to the fact that there are more possible models that we can create. However, when the dropout rate increases to a certain point the accuracy decreases, because each of our models do not have enough predictors to make accurate predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.1,16': 0.6399999999999999,\n",
       " '0.1,2': 0.63640000000000008,\n",
       " '0.1,32': 0.6399999999999999,\n",
       " '0.1,4': 0.6399999999999999,\n",
       " '0.1,64': 0.6399999999999999,\n",
       " '0.1,8': 0.64700000000000002,\n",
       " '0.3,16': 0.64459999999999995,\n",
       " '0.3,2': 0.63840000000000008,\n",
       " '0.3,32': 0.64080000000000004,\n",
       " '0.3,4': 0.62859999999999994,\n",
       " '0.3,64': 0.64439999999999997,\n",
       " '0.3,8': 0.64060000000000006,\n",
       " '0.5,16': 0.64940000000000009,\n",
       " '0.5,2': 0.57899999999999996,\n",
       " '0.5,32': 0.65240000000000009,\n",
       " '0.5,4': 0.63560000000000005,\n",
       " '0.5,64': 0.64700000000000002,\n",
       " '0.5,8': 0.63840000000000008,\n",
       " '0.7,16': 0.63739999999999997,\n",
       " '0.7,2': 0.59939999999999993,\n",
       " '0.7,32': 0.64639999999999997,\n",
       " '0.7,4': 0.62560000000000004,\n",
       " '0.7,64': 0.6532,\n",
       " '0.7,8': 0.63200000000000001,\n",
       " '0.9,16': 0.57440000000000002,\n",
       " '0.9,2': 0.53180000000000016,\n",
       " '0.9,32': 0.5766,\n",
       " '0.9,4': 0.53580000000000005,\n",
       " '0.9,64': 0.5514,\n",
       " '0.9,8': 0.5776}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CV to determine best p and b combo\n",
    "#tree_counts1 only goes to 64 trees to avoid lenghty computation times\n",
    "tree_counts1 = [2**x for x in range(1, 7)]\n",
    "combinations = list(itertools.product(probabilities, tree_counts1))\n",
    "combo_scores = dict()\n",
    "for p, b in combinations:\n",
    "    scores = []\n",
    "    train_sets = [x[0] for x in create_sets(x_train, x_test, p, b)]\n",
    "    for train, valid in KFold(5, shuffle=False).split(range(5000)):\n",
    "        train_list1 = []\n",
    "        test_list1 = []\n",
    "        ny_train = y_train[train]\n",
    "        ny_test = y_train[valid]\n",
    "        for s in train_sets:\n",
    "            nx_train = s.iloc[train]\n",
    "            nx_test = s.iloc[valid]\n",
    "            train_list1.append(nx_train)\n",
    "            test_list1.append(nx_test)\n",
    "        m = model_array(train_list1, ny_train)\n",
    "        test_predictions = ensemble_classifier(m, test_list1)\n",
    "        scores.append(metrics.accuracy_score(ny_test, test_predictions))\n",
    "    combo_scores[str(p) + ',' + str(b)] = np.mean(scores)\n",
    "combo_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65239999999999998"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the best combination found by our model is p = 0.5 and b = 16\n",
    "set_list = create_sets(x_train, x_test, 0.5, 16)\n",
    "train_list = [x[0] for x in set_list]\n",
    "test_list = [x[1] for x in set_list]\n",
    "m = model_array(train_list, y_train)\n",
    "test_predictions = ensemble_classifier(m, test_list)\n",
    "metrics.accuracy_score(y_test, test_predictions)\n",
    "#this test performance is better than what we got in part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (15pt): Random Forests\n",
    "\n",
    "We now move to a more sophisticated ensemble technique, namely random forest:\n",
    "1. How does a random forest approach differ from the dropout procedure described in Question 2? \n",
    " \n",
    "- Fit random forest models to the training set for different number of trees (say $2, 4, 8, 16, \\ldots, 256$), and evaluate the training and test accuracies of the models. You may set the number of predictors for each tree in the random forest model to $\\sqrt{p}$, where $p$ is the total number of predictors. \n",
    "\n",
    "- Based on your results, do you find that a larger number of trees necessarily improves the test accuracy of a random forest model? Explain how the number of trees effects the training and test accuracy of a random forest classifier, and how this relates to the bias-variance trade-off for the classifier. \n",
    "  \n",
    "- Fixing the number of trees to a reasonable value, apply 5-fold cross-validation to choose the optimal value for the  number of predictors. How does the test performance of random forest model fitted with the optimal number of trees compare with the dropout approach in Question 2?  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does a random forest approach differ from the dropout procedure described in Question 2?\n",
    "- The random forest approach is much different, because rather than dropping certain predictors, different decision trees are built using different samples that are obtained through bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'128': 0.70279999999999998,\n",
       " '16': 0.66800000000000004,\n",
       " '2': 0.58179999999999998,\n",
       " '256': 0.70299999999999996,\n",
       " '32': 0.68300000000000005,\n",
       " '4': 0.61319999999999997,\n",
       " '64': 0.69199999999999995,\n",
       " '8': 0.64239999999999997}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the square root of 28 is about 5, so we use 5 predictors for each tree\n",
    "tree_counts2 = [2**x for x in range(1, 9)]\n",
    "train_scores = dict()\n",
    "test_scores = dict()\n",
    "for c in tree_counts2:\n",
    "    rand_forest = RandomForestClassifier(n_estimators = c, max_features = 5, random_state = 1)\n",
    "    rand_forest.fit(x_train, y_train)\n",
    "    tr_pred = rand_forest.predict(x_train)\n",
    "    tst_pred = rand_forest.predict(x_test)\n",
    "    train_scores[str(c)] = metrics.accuracy_score(y_train, tr_pred)\n",
    "    test_scores[str(c)] = metrics.accuracy_score(y_test, tst_pred)\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'128': 1.0,\n",
       " '16': 0.998,\n",
       " '2': 0.85099999999999998,\n",
       " '256': 1.0,\n",
       " '32': 0.99980000000000002,\n",
       " '4': 0.92879999999999996,\n",
       " '64': 1.0,\n",
       " '8': 0.98140000000000005}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your results, do you find that a larger number of trees necessarily improves the test accuracy of a random forest model? Explain how the number of trees effects the training and test accuracy of a random forest classifier, and how this relates to the bias-variance trade-off for the classifier.\n",
    "- We found in our results that a larger number of trees does necessarily improve the test accuracy of a random forest, although the rate of improvement seems to slow dramatically as the number of trees increases. This happens, because there are only so many possible subsamples of the data and therefore only so many possible trees. So after a certain number of trees we will just be getting the same trees repeated in the same proportions and therefore our scores will plateau. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c29998b828>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c29f683630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot of test scores vs. number of trees\n",
    "x1, y1 = zip(*test_scores.items())\n",
    "plt.plot(x1, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing the number of trees to a reasonable value, apply 5-fold cross-validation to choose the optimal value for the number of predictors. How does the test performance of random forest model fitted with the optimal number of trees compare with the dropout approach in Question 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 0.64059820570914217,\n",
       " '10': 0.67260141466658607,\n",
       " '11': 0.68320445684268927,\n",
       " '12': 0.67320261437908491,\n",
       " '13': 0.67640173575425377,\n",
       " '14': 0.6777996570330771,\n",
       " '15': 0.66980461249247902,\n",
       " '16': 0.67300205453884188,\n",
       " '17': 0.673401015082292,\n",
       " '18': 0.67060133405880329,\n",
       " '19': 0.67380309437923847,\n",
       " '2': 0.66000021111562124,\n",
       " '20': 0.67179869549738358,\n",
       " '21': 0.67540061588184441,\n",
       " '22': 0.66359925265070052,\n",
       " '23': 0.67080357322785222,\n",
       " '24': 0.67359941578549887,\n",
       " '25': 0.66800077345086706,\n",
       " '26': 0.67020189370712313,\n",
       " '27': 0.66840045370666257,\n",
       " '28': 0.67180157434676513,\n",
       " '3': 0.66439933287463659,\n",
       " '4': 0.67520125556217681,\n",
       " '5': 0.67699789748033512,\n",
       " '6': 0.67319613696797675,\n",
       " '7': 0.6806029366182923,\n",
       " '8': 0.67740285562666314,\n",
       " '9': 0.67179893540149882}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fixing the number of trees to 64, because it's accuracy is very close to that of a forest with 128 trees, but will be less expensive\n",
    "feature_count = np.arange(1, 29)\n",
    "fscores = dict()\n",
    "for f in feature_count:\n",
    "    rand_forest = RandomForestClassifier(n_estimators = 64, max_features = f, random_state = 1)\n",
    "    scores = cross_val_score(rand_forest, x_train, y_train)\n",
    "    fscores[str(f)] = np.mean(scores)\n",
    "fscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70140000000000002"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the best number of features we found was 11 and the best practical number of trees we found was 128\n",
    "rand_forest = RandomForestClassifier(n_estimators = 128, max_features = 11, random_state = 1)\n",
    "rand_forest.fit(x_train, y_train)\n",
    "tst_pred = rand_forest.predict(x_test)\n",
    "optimal_rand_forest_score = metrics.accuracy_score(y_test, tst_pred)\n",
    "optimal_rand_forest_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (15pt): Boosting\n",
    "\n",
    "We next compare the random forest model with the approach of boosting:\n",
    "\n",
    "\n",
    "1. Apply the AdaBoost algorithm to fit an ensemble of decision trees. Set the learning rate to 0.05, and try out different tree depths for the base learners: 1, 2, 10, and unrestricted depth.  Make a plot of the training accuracy of the ensemble classifier as a function of tree depths. Make a similar plot of the test accuracies as a function of number of trees (say $2, 4, 8, 16, \\ldots, 256$).\n",
    "- How does the number of trees influence the training and test performance? Compare and contrast between the trends you see in the training and test performance of AdaBoost and that of the random forest models in Question 3. Give an explanation for your observations.\n",
    "- How does the tree depth of the base learner impact the training and test performance? Recall that with random forests, we allow the depth of the individual trees to be unrestricted. Would you recommend the same strategy for boosting? Explain your answer.\n",
    "- Apply 5-fold cross-validation to choose the optimal number of trees $B$ for the ensemble and the optimal tree depth for the base learners. How does an ensemble classifier fitted with the optimal number of trees and the optimal tree depth compare with the random forest model fitted in Question 3.4? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score as a function of depth {'1': 0.68079999999999996, '2': 0.74019999999999997, '10': 1.0}\n",
      "\n",
      "Train Score as a function of number of trees {'2': 0.91679999999999995, '4': 0.9718, '8': 0.99180000000000001, '16': 0.99680000000000002, '32': 1.0, '64': 1.0, '128': 1.0, '256': 1.0}\n",
      "\n",
      "Test Score as a function of depth {'1': 0.68079999999999996, '2': 0.74019999999999997, '10': 1.0}\n",
      "\n",
      "Test Score as a function of number of trees {'2': 0.61760000000000004, '4': 0.61399999999999999, '8': 0.60140000000000005, '16': 0.60360000000000003, '32': 0.62339999999999995, '64': 0.62119999999999997, '128': 0.64959999999999996, '256': 0.64339999999999997}\n"
     ]
    }
   ],
   "source": [
    "# is n_estimators = 100 good? I can't put none in depths_ada because I can't plot... how do I show unrestricted depth?\n",
    "boost_train_score_dep = dict()\n",
    "boost_train_score_num = dict()\n",
    "boost_test_score_dep = dict()\n",
    "boost_test_score_num = dict()\n",
    "depths_ada = [1, 2, 10]\n",
    "tree_counts3 = [2**x for x in range(1,9)]\n",
    "for i, d in enumerate(depths_ada): \n",
    "    for c in tree_counts3: \n",
    "        adaboost = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=d), n_estimators=c, learning_rate=0.05)\n",
    "        adaboost.fit(x_train, y_train)\n",
    "        sc_train = adaboost.score(x_train, y_train)\n",
    "        sc_test = adaboost.score(x_test, y_test)\n",
    "        boost_train_score_dep[str(d)] = sc_train\n",
    "        boost_train_score_num[str(c)] = sc_train\n",
    "        boost_test_score_dep[str(d)] = sc_train\n",
    "        boost_test_score_num[str(c)] = sc_test\n",
    "print(\"Train Score as a function of depth\", boost_train_score_dep)\n",
    "print(\"\")\n",
    "print(\"Train Score as a function of number of trees\", boost_train_score_num)\n",
    "print(\"\")\n",
    "print(\"Test Score as a function of depth\", boost_test_score_dep)\n",
    "print(\"\")\n",
    "print(\"Test Score as a function of number of trees\", boost_test_score_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c29f68e3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x2, y2 = zip(*boost_train_score_dep.items())\n",
    "plt.plot(x2, y2)\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.xlabel('Tree Depths');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8XXWd//HXJ/vapmkWSrd0SVkKLdC0oKLsyiYoLgOM\nqDOMFR6DP3VmwKKO64wOMI6zCIPIMjKjokARBKSCrCrQpLV7oU3bdKdpm7RNumT9/P64J+1tyHLS\n5ubm3vt+Ph59JPfcc8/9HC69757v93y/X3N3RERE+pMW7wJERCQxKDBERCQUBYaIiISiwBARkVAU\nGCIiEooCQ0REQlFgiIhIKAoMEREJRYEhIiKhZMS7gMFUUlLiFRUV8S5DRCRhLFq0aJe7l4bZN6kC\no6KigpqamniXISKSMMxsY9h91SQlIiKhKDBERCQUBYaIiIQS08Aws0vN7G0zqzWzeb3sc76ZLTGz\nlWb2StT2OjNbHjynjgkRkTiLWae3maUDdwOXAFuAajN7yt1XRe1TBNwDXOrum8ysrNthLnD3XbGq\nUUREwovlFcYcoNbd17t7K/AIcHW3fa4H5rv7JgB3r49hPSIichxiGRhjgc1Rj7cE26JNA0aZ2ctm\ntsjMPh31nAMvBNvnxrBOEREJId7jMDKAWcBFQC7wupm94e5rgHPdfWvQTPW8mb3l7q92P0AQJnMB\nJkyYMISli/TviT9v4WBrJ5XlBUwrK2RkXma8SxI5ZrEMjK3A+KjH44Jt0bYAu919P7DfzF4FZgJr\n3H0rRJqpzOwJIk1c7woMd78PuA+gqqpKC5TLsPH71Tv48i+XHrWttDCbaeUFVJYVMrWsgGnlhVSW\nFTAqPytOVYqEF8vAqAYqzWwSkaC4lkifRbQngR+ZWQaQBZwN/NDM8oE0d28Kfv8g8J0Y1ioyqPYe\nbOOrTyznpPJCfnzDLDbs2s/a+ibW7GhmbX0zj9ZsZn9rx+H9SwqyqSwrYFp5AVPLC5lWVkBleSHF\nChIZRmIWGO7ebma3AAuAdOBBd19pZjcFz9/r7qvN7DlgGdAJ3O/uK8xsMvCEmXXV+HN3fy5WtYoM\ntu89s5qdTS385NNVVJTkU1GSzwUnH7kJ0N3ZtvcQa3c0sXZH8+EweXzxVppb2g/vNzo/i8rgimRa\neQFTg5+jC7LjcVqS4sw9eVpxqqqqXHNJSby9umYnn35wITedN4V5l508oNe6O+/sOxS5EokKk7U7\nmmmKCpLi/CwqywoifSPlkeatyrJCSgqyCP6hJRKKmS1y96ow+8a701skqTS3tHP7/OVMLs3nSxdX\nDvj1ZsaYkbmMGZnLedOOTCDq7uzY18KaHU2srW+mNrgieXLJNpoOHQmSUXmZVJYVBlclQZiUF1Ba\nkK0gkeOmwBAZRP/y29Vs23uQx256DzmZ6YN2XDPjhJE5nDAyhw90C5L6phbW7mg+HCZrdzTxm6Xb\n2BcVJEV5mVSWHWnS6mriKi1UkEh4CgyRQfKndbv4vzc2ceO5k5g1sXhI3tPMKB+RQ/mIHM6tLDm8\n3d3Z2RwJkrU7mlhT30ztjmZ+u2I7v1jYdni/ETkZVJYf3T9SWVZI+QgFibybAkNkEBxobWfe48uZ\nODqPf/jgSfEuBzOjrDCHssIc3jf16CDZ1dx6uF+kq7P9uRXv0HjgyDjbwpyMI01awR1b08oLOGFE\njoIkhSkwRAbBXQveZlPDAR6Zew65WYPXFDXYzIzSwmxKC7N575SSo57b1XVFEhUmz6/awSPVUUGS\nncHU6P6R4OeYkQqSVKDAEDlONXUN/M+f6rjhnImcM3l0vMs5ZiUF2ZQUZPOeKUefw+7mlkjfSP2R\nO7defGsnv6rZcnifguwMppQVBONHIlcklWUFjC3KVZAkEQWGyHE41NbBbY8t48SRuQO+hTZRjC7I\nZnRB9rvCsHF/K2vrI53ttcHPl9fs5NFFR4IkPyv9cJNW123AlWWFjC3KJS1NQZJoFBgix+GHz69h\n/a79/N+NZ5OfnVp/nUblZzFnUjFzJh3dwb/nQGtwNXIkTF5ds5PHooIkLwiS6OlRKssKGTdKQTKc\npdb/4SKDaMnmPfzktfVcO3v8UXcopbqivCxmVxQzu+LoINl7oC3SPxKEydr6Jv5Uu5v5i49MMZeT\nmRYJkbLI+JFpwZiS8aPyFCTDgAJD5Bi0tHdw66NLKSvM4atXnBLvchLCyLxMqiqKqeoeJAfbqO3q\nHwn6Sl5fv5v5fz46SKaUFgTNWoWHO93HF+eRriAZMgoMkWPwoxdrWVvfzEOfnc2IHE1ZfjxG5mYy\na+IoZk0cddT2pkNtkVHtUYMSF25o4NdLth3eJzsjCJKjpkgpYOLofAVJDCgwRAZoxda93PPyOq45\na+xREwrK4CrMyeSsCaM4a8K7g2Tdzv1HdbbX1DXyZFSQZGWkMbkk/0j/SHmkaWticR4Z6bFcNy65\nKTBEBqCto5NbH1tGcX4W37jy1HiXk5IKczI5Y3wRZ4wvOmp7c0s766Jv/61vZvGmRp5aGhUk6WlM\nLs0/urO9vJCJo/PIVJD0S4EhMgD//fI6Vm/fx49vmEVRntaqGE4KsjOYOb6Imd2C5EBre9BH0sya\n+iZqdzSzbMtenlm+na7JujPTjcklBUcNSqwsK6CiJF9BEkWBIRLS2+808V8vruXKGWP40PQT4l2O\nhJSXlcGMcUXMGPfuIFkfNG11XZWs2LqXZ6OCJCPNmBQ0bR2+KikvoGJ0PlkZqRckCgyRENo7Orn1\nsaWMyMnk21dNj3c5MgjysjI4bexIThs78qjtB1s7WLez+XD/yNr6ZlZu28uzK44OkoqS/HfdtVVR\nkkd2xvCdGuZ4KTBEQvjJaxtYtmUvP7r+TK12l+Rys9J7DJJDbUeCpGtQ4tvvNLFg5Tt0BkGSnmZU\njM47siZJECaTS/OTIkgUGCL9qK1v5ocvrOFD08u54vQx8S5H4iQnM53pJ45k+onvDpINu47ctdXV\nV/L86h10BEmSZlAxOv/w1ChdPyeX5g/quimxpsAQ6UNHp3PbY0vJzUznux85TRPpybvkZKZzypgR\nnDJmxFHbW9ojQdK1JknXvFsvrK4/Kkgmju66a+tImEwpLRiWQaLAEOnD//ypjsWb9vBvn5xJWWFO\nvMuRBJKdkc7JJ4zg5BOODpLW9k7qdged7VHTyb/0Vj3tQZCYwYTiI01bXWEypbQgrtPnKzBEelG3\naz93LXiLC08u46Nnjo13OZIksjLSmFZeyLTywqO2t7Z3snH3/sNXIl13br2ypp62jiNBMn5U3uEV\nErs626eU5ZOXFfuvcwWGSA86O52vPL6MzLQ0/vmjaoqS2MvKSAtGpBdyeVRfWVtHECQ7mlkTXJFE\nZgDeRWtHJxBZanfpNz8Y8/9PFRgiPfjZmxt5c0MDd3zsdMaMzI13OZLCMtPTmFpWyNSyQi47/cj2\n9o5ONjYcYO2OJvYebBuSf9QoMES62dxwgO//9i3eX1nCJ6vGx7sckR5lpEcmXpxSWjBk75l6QxVF\n+uDufPWJ5Rjw/WtOV1OUSBQFhkiUX1Zv5rW1u5h3+SmMG5UX73JEhhUFhkhg+96D/PMzqzlncjF/\nOWdCvMsRGXYUGCIETVHzl9PW2ckdH5uh5UBFeqDAEAGe+PNWXnp7J7d+6GQmjs6Pdzkiw5ICQ1Je\n/b5DfPs3q5g1cRSffW9FvMsRGbYUGJLS3J2v/3oFB9s6uONjM7QOtEgfFBiS0p5etp3frdrB310y\njallQ3c/u0giimlgmNmlZva2mdWa2bxe9jnfzJaY2Uoze6Xbc+lm9mczezqWdUpq2t3cwjefWsnM\ncSP5m3MnxbsckWEvZiO9zSwduBu4BNgCVJvZU+6+KmqfIuAe4FJ332RmZd0O80VgNTACkUH2zadW\n0nSojTs/fg4ZWrdZpF+x/FsyB6h19/Xu3go8AlzdbZ/rgfnuvgnA3eu7njCzccAVwP0xrFFS1HMr\n3uHpZdv5fxdWctIJhf2/QERiGhhjgc1Rj7cE26JNA0aZ2ctmtsjMPh313L8DtwGdMaxRUtCeA618\n/dcrOHXMCG46f0q8yxFJGPGefDADmAVcBOQCr5vZG0SCpN7dF5nZ+X0dwMzmAnMBJkzQ6Fzp33d+\ns4o9B1r56V/PJlNNUSKhxfJvy1YgeqrPccG2aFuABe6+3913Aa8CM4H3AVeZWR2RpqwLzez/enoT\nd7/P3avcvaq0tHSwz0GSzItv7WD+n7dy8/lT3rU2s4j0LZaBUQ1UmtkkM8sCrgWe6rbPk8C5ZpZh\nZnnA2cBqd7/d3ce5e0Xwuhfd/VMxrFVSwL5DbXx1/gqmlRdwy4VT412OSMKJWZOUu7eb2S3AAiAd\neNDdV5rZTcHz97r7ajN7DlhGpK/ifndfEauaJLV975nV1Dcd4sc3vI/sjPitiyySqGLah+HuzwLP\ndtt2b7fHdwF39XGMl4GXY1CepJDX1u7kkerNfP68ycwcXxTvckQSknr8JOk1t7Qz7/HlTC7N58sX\nT4t3OSIJK953SYnE3B2/fYttew/y2E3vISdTTVEix0pXGJLUXl+3m/99YyN/9d5JzJpYHO9yRBKa\nAkOS1oHWdr7y+DImFOfxDx9SU5TI8VKTlCStf12whk0NB/jF584hL0v/q4scL11hSFJatLGBh/60\ngU+dM4H3TBkd73JEkoICQ5LOobYObn1sGSeOzGXeZafEuxyRpKHrdEk6//7CWtbv3M/Dfz2Hgmz9\nLy4yWHSFIUll6eY93PfqOv6iajwfmKa5xUQGkwJDkkZLewe3PraUssIcvnalmqJEBpuu1yVp3P1i\nLWt2NPPgZ6sYkZMZ73JEko6uMCQprNy2l3teXsc1Z47lwpPL412OSFLqNzDM7GYz08IBMmy1dXRy\n66PLKMrL4hsfPjXe5YgkrTBXGBOBxWb2czO7ONYFiQzUvS+vY9X2ffzTR06jKC8r3uWIJK1+A8Pd\n5wGVwM+Am8xsrZl9x8wqYlybSL/W7GjiP19cyxUzxnDpaSfEuxyRpBaqD8PdO4G64E8nMAZ40sy+\nH7PKRPrR3tHJrY8upTAnk+9cNT3e5YgkvX7vkjKzvwU+A+wDHgC+5u4tZpYG1AK3x7ZEkZ498IcN\nLN2yl/+87kxGF2THuxyRpBfmttoTgevcfV30RnfvNLOrYlOWSN/W7WzmB8+v4YOnlvPhGWPiXY5I\nSgjTJPVrYEfXAzMrNLMqAK2/LfHQ0enc9tgycjPT+aePnIaZxbskkZQQJjDuAw5EPd4P/Dg25Yj0\n76d/qmPRxka+ceWplI3IiXc5IikjTGCkBZ3ewOEOcA2jlbjYuHs/dy54iwtOKuWas8bGuxyRlBIm\nMDYEg/fSzSwt6ASvi3FdIu/S2el85fFlZKal8b1rTldTlMgQCxMYnwcuItKPsQM4D/hcLIsS6cnP\nFm7ijfUNfO2KUxgzMjfe5YiknH7vknL3HcDHh6AWkV5taTzAvzy7mnOnlvAXs8fHuxyRlBRmHEY2\n8FlgOnC4h9Hd58auLJEj3J3b5y/Hge+rKUokbsI0ST0MVABXAm8CU4BDMaxJ5CiP1mzhtbW7uP2y\nkxlfnBfvckRSVpjAmObutwPN7v4AcCkwJ7ZliUS8s/cQ331mFWdPKuYvz54Y73JEUlqYwGgLfu4x\ns1OAQqAsdiWJRLg7X31iOW0dndzxsRmkpakpSiSewkwN8oCZjQK+CSwA8oBvxLQqEeDXS7by4lv1\nfP2KU6goyY93OSIpr8/AMLN0YJe7NwIvAROGpCpJefVNh/jWU6s4a0IRf/W+SfEuR0Top0nK3TuA\nrw5RLSJApCnqH3+9goNtHdz58ZmkqylKZFgI04fxOzP7kpmNMbMRXX9iXpmkrGeWb2fByh18+eJp\nTC0riHc5IhIIExifAv4eWAisDP6EmqXWzC41s7fNrNbM5vWyz/lmtsTMVprZK8G2HDNbaGZLg+3f\nDnc6kuh2N7fwzSdXMmPcSD73fjVFiQwnYUZ6H9Ow2qD/427gEmALUG1mT7n7qqh9ioB7gEvdfZOZ\ndd191QJc6O7NZpYJ/MHMfuvubxxLLZI4vvWbVew71MbPP34OGemhFoQUkSESZqT39T1td/ef9/PS\nOUCtu68PjvMIcDWwKmqf64H57r4pOGZ98NOB5mCfzOCP91erJLYFK9/hN0u38XeXTOOkEwrjXY6I\ndBPmttr3R/2eA1wILAL6C4yxwOaox1uAs7vtMw3INLOXiYzv+A93fxgOX6EsAqYCd7v7myFqlQS1\n50ArX//1Ck4ZM4Kbz58S73JEpAdhmqRujn4cjMnoLywG8v6ziMyGmwu8bmZvuPua4A6tM4JmqyfM\n7LSeVvgzs7nAXIAJE3TXb6L6ztOraNjfykOfnU2mmqJEhqVj+ZvZBEwOsd9WILr/Y1ywLdoWYIG7\n73f3XcCrwMzoHdx9D5ExIJf29Cbufp+7V7l7VWlpachTkOHkpbfqmb94KzefN4XTxo6Mdzki0osw\nfRhPcKT/II3IrLVPhjh2NVBpZpOIBMW1RPosoj0J/MjMMoAsIk1WPzSzUqDN3feYWS6RjvM7Qryn\nJJh9h9q4ff5yKssK+MJFU+Ndjoj0IUwfxo+ifm8HNrp7XX8vcvd2M7uFyHQi6cCD7r7SzG4Knr/X\n3Veb2XPAMqATuN/dV5jZDOCnQT9GGvArd396QGcmCeH7z66mvukQ997wPrIz0uNdjoj0IUxgrAXq\n3f0QgJnlmtl4d9/cz+tw92eBZ7ttu7fb47uAu7ptWwacGaI2SWB/WLuLXyzczOc/MJkzxhfFuxwR\n6UeYPoz5RP7136UTeDw25Uiq2N/SzlceX8bkkny+fMm0eJcjIiGECYwMd2/teuDuLUB27EqSVHDH\nc2+xbe9B7vz4DHIy1RQlkgjCBMZuM7u864GZXQk0xK4kSXZvrt/Nw69v5LPvraCqojje5YhISGH6\nMG4Gfm5mdwePdxKZX0pkwA62dnDb48uYUJzHrR86Kd7liMgAhBm4twaoCgbQdY2LEDkm//q7t9m4\n+wA//9zZ5GWF+feKiAwX/TZJmdl3zazI3fcE4yJGafZYORaLNjby4B838JdnT+C9U0riXY6IDFCY\nPowro68qgtX3Phy7kiQZHWrr4LbHlnLiyFxuv/yUeJcjIscgTGCkm1lW1wMzyyEyKlsktP/4/VrW\n7dzP9645nYJsNUWJJKIwf3MfAZ43sweDx3/N4E0+KClg2ZY93Pfqej5ZNY7zpmm+L5FEFabT+3tm\ntgy4ONh0p7s/E9uyJFm0tndy66PLKCnI4mtXnBrvckTkOIRqGwjmcXoawMzOMbP/cPcvxrQySQo/\neqmWt3c08cBnqhiZmxnvckTkOIQKDDM7HbgO+AtgG5oaREJYuW0v97xUy0fPHMtFp5THuxwROU69\nBoaZTSYSEtcRWS71l0Cmu7+/t9eIdGnr6OS2x5ZRlJfFN65UU5RIMujrCqMWeA24Jhi8h5l9YUiq\nkoT341fWsXLbPu791FmMytdNdSLJoK/baj9JZBqQF8zsHjM7D7ChKUsS2ZodTfzn72u54vQxXHra\nmHiXIyKDpNfAcPfH3P3jRFbYewOYB5Sb2X+Z2YVDVaAklvaOTm59bBn52el8++rp8S5HRAZRvwP3\n3L3J3R9298uACcBq4Jsxr0wS0oN/3MDSzXv41lXTKSnQLPgiySTMSO/D3H2Xu9/j7ufFqiBJXOt3\nNvOD363hklPLuWrmifEuR0QG2YACQ6Q3nZ3ObY8tIzsjjX/+yGmYqbtLJNkoMGRQ/PT1Omo2NvKN\nD0+nbEROvMsRkRhQYAxDnZ2Ou8e7jNA27t7Pnc+9zfknlfKxs8bGuxwRiZF+R3qbWSPQ/dtrL1AD\n3OrudTGoK2W1tnfygTtfosOd2RWjqJpYzJxJxZx8QiEZ6cMv3zs7nXmPLyc9zfjeR09XU5RIEgsz\nNcjdwHaOzFB7HVABLAUeAi6ISWUpavnWvbyz7xBzJhWzdPNenl3+DgD5WemcNTESILMrRnHGhKJh\nsWLdzxdu4vX1u/n+NadzYlFuvMsRkRgK843zYXefGfX4HjNb4u63mdltsSosVdXUNQBw9/VnUVqY\nzbY9B6mua6CmrpHqugb+/fdrcIeMNGP62JHMnjiKqopIiIwe4ttYt+45yPefXc37po7m2tnjh/S9\nRWTohQmMg2Z2jbvPBzCza4CW4LnOmFWWoqrrGphUkk9pYeTL/8SiXK4+YyxXnxHpG9h7sI3FGxsP\nh8jDb2zk/j9sAGByaT6zJxZTVTGKOZOKmVCcF7MmIndn3uPLcOBfrpmhpiiRFBAmMD4F/JeZ3U+k\nL2MhcIOZ5QFfimVxqaaz06nZ2MgHT+19ZteRuZlccHIZF5xcBkBLewcrtu5l4YZGauoaeG7lO/yy\nZjMApYXZMesHeXTRFl5bu4tvXzWd8cV5g3JMERnewiygVAtc1svTrwxuOamtdmczew60UVVRHPo1\n2RnpzJpYzKyJxcAUOjud2p3NLNzQQE1dA9V1jYPeD/LO3kN89+lVzJlUzA3nTBzw60UkMYW5S6qE\nyLKsFdH7u/vc2JWVmqqD/os5AwiM7tLSjGnlhUwrL+RTwZf5YPaDuDtfe2I5bR2d3PmxGaSlqSlK\nJFWE+eflk0QmH/wD0BHbclJb9YYGSgqymTh6cJt4BrMf5Mkl2/j9W/V8/YpTqCjJH9Q6RWR4CxMY\n+e7+9zGvRKiua2TOpFEx70DuqR9k+Za9VNf13Q9y8phCvvWblZw5oYi/et+kmNYoIsNPmMD4rZl9\n0N1/F/NqUti2PQfZuucgN5479F/E2RnpVFUUB30nkX6QtfXNwRXI0f0gWRlp3PXxGaSrKUok5YQJ\njJuAr5jZAaCVyCJK7u7H3tAu73K4/2JS/P+zpqUZJ51QyEknvLsfpLQgm6llhXGuUETiIcw9liVA\nJjASKA0el4Y5uJldamZvm1mtmc3rZZ/zzWyJma00s1eCbePN7CUzWxVs/2K400lcNXWN5Gelc/IJ\nw/PLuKsf5L1TS+JdiojESa9XGGZW6e5riay415NlfR3YzNKJTCtyCbAFqDazp9x9VdQ+RcA9wKXu\nvsnMyoKn2oG/d/fFZlYILDKz56Nfm2yq6xo4a+KoYTlflIgI9N0kNQ+4kciXfncOfKCfY88Bat19\nPYCZPQJcDUR/6V8PzHf3TQDuXh/83E5k/ircvcnMVgNju702aew90MbbO5q4/HStfy0iw1evgeHu\nNwa/XujubdHPmVlmiGOPBTZHPd4CnN1tn2lAppm9DBQC/+HuD3d7rwrgTODNEO+ZkBZtasAdqipG\nxbsUEZFehWn/6OmLerC+vDOAWcAVwIeAfzSzaV1PmlkB8DjwJXff19MBzGyumdWYWc3OnTsHqayh\nVV3XSEaaceZ4BYaIDF999WGUAWOAXDM7ncjdUQAjgDAjy7YC0VOYjgu2RdsC7Hb3/cB+M3sVmAms\nCa5iHgd+1jXxYU/c/T7gPoCqqqrEWXUoSvWGBk4bO5LcrPR4lyIi0qu++jCuIDIlyDgi/RhdgdEE\n/GOIY1cDlWY2iUhQXEukzyLak8CPzCwDyCLSZPVDi4xcewBY7e7/FvJcEtKhtg6WbdnLZ99XEe9S\nRET61FcfxkPAQ2b2SXf/1UAP7O7tZnYLsABIBx5095VmdlPw/L3uvtrMniNyx1UncL+7rzCzc4Eb\ngOVmtiQ45Ffd/dmB1jHcLduyl9aOTqomqjlKRIa3MAP3ysxshLvvM7N7gbOA29399/29MPiCf7bb\ntnu7Pb4LuKvbtj9w5IomqXUN2BvIDLUiIvEQptN7bhAWHyTSp/E54M7YlpU6auoamFpWQHF+VrxL\nERHpU5jA6OpIvhx42N2Xhnyd9KMjWDBptm6nFZEEEOaLf6mZPQtcSWQiwgKOhIgchzU7mmg61M5s\nNUeJSAII04fxV0TGStS6+4FgQaUb+3lNwujodBZvamRUXhZTywqG9L27+i8UGCKSCPq9wnD3DmAy\ncHOwKTfM6xKFATc88Ca/WLhpyN+7uq6RE0bkMG5U7pC/t4jIQPX7xW9mPwIuAD4VbNoP3Nv7KxJL\nWpoxpbSAtfXNQ/q+7k71hgaqKmK/YJKIyGAIc6XwXnf/PHAIwN0biAyySxqVZQWsG+LA2NJ4kHf2\nHRoW61+IiIQRJjDazCyNoKPbzEYTGWSXNCrLC9m65yDNLe1D9p6Hx19MVGCISGLoNTCC6TogMi3I\n40CpmX0b+ANwxxDUNmSmlEY6u4fyKqO6rpHCnAxOGqYLJomIdNfXXVILgbPc/WEzWwRcTKSP+BPu\nvmJIqhsileWRwKitb2bm+KIhec+augZmTRyltbFFJGH0FRiHv8ncfSWwMvblxMfE4jwy023IOr4b\n97eytr6Zj5w5dkjeT0RkMPQVGKVm9ne9PZlMs8hmpKcxqSSf2iEKjJqNjYDGX4hIYukrMNKBAlJk\nEsCpZQWs2tbjGk2Drrqugaz0NGaMGzkk7yciMhj6Cozt7v6dIaskzqaWFfLcinc41NZBTmZsFzKq\nrmtgxriRMX8fEZHB1NdttSlxZdFlalkBnQ4bdu2P6fscbO1g+Za9ms5cRBJOX4Fx0ZBVMQxUlh25\nUyqWlmzeQ3unM2eSZqgVkcTSa2AEI7pTxqSSfNKMmN8pVV3XgBnMmqArDBFJLEkzieDxyslMZ0Jx\nXswH71XXNXBSeSEj8zJj+j4iIoNNgRFlalkha+ubYnb89o5OFm9spEoLJolIAlJgRJlUksfG3Qdw\nj836UG+908T+1g6NvxCRhKTAiFJWmENLeydNMZqEUAsmiUgiU2BEKS3MBmBXU0tMjl9d18DYolxO\nLNKCSSKSeBQYUUoKIoGxMwaB4e5U1zUyW/0XIpKgFBhRuq4wdjYPfmBs3H2AnU0tGrAnIglLgRHl\ncGDE4Aqjq/9CK+yJSKJSYEQpys0kI83YFYMrjOq6BkbmZjI1WKxJRCTRKDCipKUZowuyYnKFURP0\nX6RpwSQRSVAKjG5KC7MHPTB2Nbewftd+9V+ISEJTYHRTWpA96J3eNYfHX+gOKRFJXAqMbkoLs9nV\n1Dqox6zM08C1AAANCUlEQVSuayQ7I43TxmrBJBFJXAqMbkoKstnV3EJn5+BND1Jd18DM8UVkZ2jB\nJBFJXAqMbkoLs2nvdPYcbBuU4+1vaWfltn3MUf+FiCS4mAaGmV1qZm+bWa2Zzetln/PNbImZrTSz\nV6K2P2hm9Wa2IpY1djfYYzH+vGkPHZ2uGWpFJOHFLDDMLB24G7gMOBW4zsxO7bZPEXAPcJW7Twc+\nEfX0/wCXxqq+3pQG04MM1liM6roG0gxmTVRgiEhii+UVxhyg1t3Xu3sr8Ahwdbd9rgfmu/smAHev\n73rC3V8FhnzVv5JBvsKormvg5BNGUJijBZNEJLHFMjDGApujHm8JtkWbBowys5fNbJGZfTqG9YQy\nmE1SbR2d/HnTHk0HIiJJIWMYvP8s4CIgF3jdzN5w9zVhD2Bmc4G5ABMmTDjuggqzM8jOSBuUsRir\ntu3jYFuH+i9EJCnE8gpjKzA+6vG4YFu0LcACd9/v7ruAV4GZA3kTd7/P3avcvaq0tPS4CgYws2As\nxvEHhhZMEpFkEsvAqAYqzWySmWUB1wJPddvnSeBcM8swszzgbGB1DGsKpWSQRntX1zUwoTiP8hE5\ng1CViEh8xSww3L0duAVYQCQEfuXuK83sJjO7KdhnNfAcsAxYCNzv7isAzOwXwOvASWa2xcxujFWt\n3Q3GfFLuTk1do5qjRCRpxLQPw92fBZ7ttu3ebo/vAu7q4bXXxbK2vpQWZrN4Y+NxHWP9rv3s3t+q\nAXsikjQ00rsHJQXZNBxopb2j85iPUb0h0n+hGWpFJFkoMHpQWpiNOzTsP/ZJCKvrGinOz2JKaf4g\nViYiEj8KjB50jfauP45+jJqNDVRNHIWZFkwSkeSgwOhB1+C9Y50epH7fITbuPqABeyKSVBQYPei6\nwjjWO6Wq6yId5uq/EJFkosDoQUlhFsAxj8WormsgNzOd6SeOGMyyRETiSoHRg7ysDAqyM47jCqOB\nMycUkZmu/7wikjz0jdaL0sJsdjUP/C6ppkNtrN6+T81RIpJ0FBi9KCnIYmfToQG/bvGmPXQ6GrAn\nIklHgdGLY50epHpDA+lpxpkTimJQlYhI/CgwelFacIyBUdfA9BNHkJ8d75njRUQGlwKjF6WF2ew7\n1E5Le0fo17S2d7Jk8x6qJqo5SkSSjwKjFyWH1/YO3/G9fOteWto7mTNJM9SKSPJRYPTiWJZqrQkW\nTJqlKwwRSUIKjF4cS2BU1zUwqST/8GtFRJKJAqMXA51PqrPTqdnYyGwtmCQiSUqB0YvR+QO7wqjd\n2cyeA20asCciSUuB0YusjDSK8jJDB0Z10H+hAXsikqwUGH0YyFiM6g0NlBRkM3F0XoyrEhGJDwVG\nH0oKskP3YVTXNTJnkhZMEpHkpcDoQ2lhdqgpzv+8qZGtew5qwJ6IJDUFRh/CzCdVXdfApx9YyNii\nXK6cMWaIKhMRGXoKjD6UFmZzoLWD/S3tPT7/ypqd3PDAm5SOyObRm95D2YicIa5QRGToKDD6cGR6\nkHdfZfx2+Xb+5qfVTC4p4Feffw8nFuUOdXkiIkNKgdGH3kZ7P1qzmb/9+WJmjCviF3PPORwsIiLJ\nTHNw96G04N2B8dAfN/Dt36zi/ZUl/PiGWeRl6T+hiKQGfdv14fAVRnML7s5/vVjLvz2/hg9NL+c/\nrzuT7Iz0OFcoIjJ0FBh9KM7PIs0iVxjfe3Y1P3ltA9ecNZY7PzaDjHS15olIalFg9CE9zSjOz+ah\nP9bR3NLOZ94zkW9+eDppaRqcJyKpR/9M7kdpYTbNLe3ccsFUvnWVwkJEUpeuMPrx+Q9M5mBbB9fN\nmRDvUkRE4kqB0Y+PnDk23iWIiAwLMW2SMrNLzextM6s1s3m97HO+mS0xs5Vm9spAXisiIkMnZlcY\nZpYO3A1cAmwBqs3sKXdfFbVPEXAPcKm7bzKzsrCvFRGRoRXLK4w5QK27r3f3VuAR4Opu+1wPzHf3\nTQDuXj+A14qIyBCKZWCMBTZHPd4SbIs2DRhlZi+b2SIz+/QAXisiIkMo3p3eGcAs4CIgF3jdzN4Y\nyAHMbC4wF2DCBN3JJCISK7G8wtgKjI96PC7YFm0LsMDd97v7LuBVYGbI1wLg7ve5e5W7V5WWlg5a\n8SIicrRYBkY1UGlmk8wsC7gWeKrbPk8C55pZhpnlAWcDq0O+VkREhlDMmqTcvd3MbgEWAOnAg+6+\n0sxuCp6/191Xm9lzwDKgE7jf3VcA9PTaWNUqIiL9M3ePdw2Dxsx2AhsH8JISYFeMyhmOdL7JTeeb\n3GJ1vhPdPVR7flIFxkCZWY27V8W7jqGi801uOt/kNhzOV5MPiohIKAoMEREJJdUD4754FzDEdL7J\nTeeb3OJ+vindhyEiIuGl+hWGiIiElLKBkQrTp5tZnZktD6aPrwm2FZvZ82a2Nvg5Kt51Hisze9DM\n6s1sRdS2Xs/PzG4PPu+3zexD8an62PVyvt8ys63BZ7zEzC6Pei5hz9fMxpvZS2a2Klj64IvB9qT8\nfPs43+H1+bp7yv0hMhhwHTAZyAKWAqfGu64YnGcdUNJt253AvOD3ecAd8a7zOM7vA8BZwIr+zg84\nNfics4FJweefHu9zGITz/RbwDz3sm9DnC4wBzgp+LwTWBOeUlJ9vH+c7rD7fVL3CSOXp068Gfhr8\n/lPgI3Gs5bi4+6tAQ7fNvZ3f1cAj7t7i7huAWiL/HySMXs63Nwl9vu6+3d0XB783EZkyaCxJ+vn2\ncb69icv5pmpgpMr06Q68EEwdPzfYVu7u24Pf3wHK41NazPR2fsn8mX/BzJYFTVZdTTRJc75mVgGc\nCbxJCny+3c4XhtHnm6qBkSrOdfczgMuAvzWzD0Q/6ZFr26S9TS7Zzy/w30SaVs8AtgM/iG85g8vM\nCoDHgS+5+77o55Lx8+3hfIfV55uqgRF6+vRE5u5bg5/1wBNELll3mNkYgOBnfe9HSEi9nV9Sfubu\nvsPdO9y9E/gJR5olEv58zSyTyJfnz9x9frA5aT/fns53uH2+qRoYST99upnlm1lh1+/AB4EVRM7z\nM8FunyEyxXwy6e38ngKuNbNsM5sEVAIL41DfoOr68gx8lMhnDAl+vmZmwAPAanf/t6inkvLz7e18\nh93nG++7A+L1B7icyJ0I64CvxbueGJzfZCJ3USwFVnadIzAa+D2wFngBKI53rcdxjr8gcpneRqQN\n98a+zg/4WvB5vw1cFu/6B+l8/xdYTmSJgKeAMclwvsC5RJqblgFLgj+XJ+vn28f5DqvPVyO9RUQk\nlFRtkhIRkQFSYIiISCgKDBERCUWBISIioSgwREQkFAWGJDwzczP7QdTjfzCzbw3Ssf/HzD4+GMfq\n530+YWarzeylqG2nR81S2mBmG4LfX4h1PSI9UWBIMmgBrjGzkngXEs3MMgaw+43A59z9gq4N7r7c\n3c/wyPQuTwG3Bo8vPo73ETlmCgxJBu1Elq/8cvcnul8hmFlz8PN8M3vFzJ40s/Vm9i9m9pdmttAi\na4hMiTrMxWZWY2ZrzOzK4PXpZnaXmVUHE8N9Puq4r5nZU8CqHuq5Ljj+CjO7I9j2DSIDtx4ws7vC\nnLCZXWxmL5vZ00QGdmFmnwnqX2Jm95hZWrD9MjN73cwWm9kvg5H/BPWvCuq/I8z7SmrTv0wkWdwN\nLDOzOwfwmpnAKUSmDF8P3O/uc4LFa74AfCnYr4LIHD5TgJfMbCrwaWCvu882s2zgj2b2u2D/s4DT\nPDLt9GFmdiJwBzALaAR+Z2YfcffvmNmFRNY9qBlA/VVE1nHZZGanEZk64r3u3m5m9xGZOuIFIutG\nXOTuB8zsa8AXzewBIiOJp7u7m1nRAN5XUpQCQ5KCu+8zs4eB/wccDPmyag+myjazdUDXF/5y4IKo\n/X7lkcnf1prZeuBkInNzzYi6ehlJZD6fVmBh97AIzAZedvedwXv+jMiiSL8OWW93r7v7puD3i4Pj\n10SmJSKXyPTXB4gstvOnYHsW8AciIdkJ/MTMngGePsYaJIUoMCSZ/DuwGHgoals7QdNr0ESTFfVc\nS9TvnVGPOzn670b3+XMcMOAL7r4g+gkzOx/Yf2zlD1j0+xjwoLv/Y7d6Pgo85+43dH+xmVUBlwCf\nAG4mEoIivVIfhiQNd28AfkWkA7lLHZEmIICrgMxjOPQnzCwt6NeYTGSytwXAzcGU1JjZtK6+gT4s\nBM4zsxIzSweuA145hnp68gLwya6OfzMbbWYTgD8F7zk52J5vZpXBTMYj3P1pIn0/Zw5SHZLEdIUh\nyeYHwC1Rj38CPGlmS4HnOLZ//W8i8mU/ArjJ3Q+Z2f1E+jYWB1NT76Sf5W7dfbuZzQNeInJF8Iy7\nD8r08u6+3My+TWSFxTQiM9re5O7VZnYj8EuLTOUP8FUizXbzg/6XNODvBqMOSW6arVZEREJRk5SI\niISiwBARkVAUGCIiEooCQ0REQlFgiIhIKAoMEREJRYEhIiKhKDBERCSU/w8+9nfuAZaQAwAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c29f89bac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x3, y3 = zip(*boost_test_score_num.items())\n",
    "plt.plot(x3, y3)\n",
    "plt.ylabel('Testing Accuracy')\n",
    "plt.xlabel('Number of Trees');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the number of trees influence the training and test performance? \n",
    "<tr>\n",
    "As the number of trees increases, the training performance increases in accuracy. It starts 0.912 accuracy with 2 trees and reaches 1 when number of trees are greater than 64. The testing performance follows a similar increasing trend, but starts with 0.62 accuracy with 2 trees and reaches 0.6588 accuracy with 256 trees. A similar trend is found in the results on question 3, yet, for example, the testing accuracy is able to reach 0.7 accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the tree depth of the base learner impact the training and test performance? \n",
    "<tr>\n",
    "As the depth of the base learner increases, the training accuracy increases steadily from 0.68 when depth is set to 1 to 1 when depth is set to 10. The testing accuracy instead increases at a similar rate, starting at 0.68 with the depth set at 1 to reaching 1 when depth is set to 10 as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply 5-fold cross-validation to choose the optimal number of trees for the ensemble and the optimal tree depth for the base learners. How does an ensemble classifier fitted with the optimal number of trees and the optimal tree depth compare with the random forest model fitted in Question 3.4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1,128': 0.65020180734164168,\n",
       " '1,16': 0.59920347035696775,\n",
       " '1,2': 0.60020387051703172,\n",
       " '1,256': 0.66280085175557035,\n",
       " '1,32': 0.61520003684927216,\n",
       " '1,4': 0.60020387051703172,\n",
       " '1,64': 0.64160244433504809,\n",
       " '1,8': 0.60020387051703172,\n",
       " '10,128': 0.63200388068896618,\n",
       " '10,16': 0.59320514776654065,\n",
       " '10,2': 0.61460843330137804,\n",
       " '10,256': 0.64440116574207618,\n",
       " '10,32': 0.61000299400335678,\n",
       " '10,4': 0.60640947026292535,\n",
       " '10,64': 0.61660635477212467,\n",
       " '10,8': 0.59099970923621248,\n",
       " '2,128': 0.67520029594571651,\n",
       " '2,16': 0.64300348436736809,\n",
       " '2,2': 0.62100139816118294,\n",
       " '2,256': 0.67740069648962697,\n",
       " '2,32': 0.64640076654162859,\n",
       " '2,4': 0.62100139816118294,\n",
       " '2,64': 0.66139981172325057,\n",
       " '2,8': 0.6254031588654646}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_counts4 = [2**x for x in range(1,9)]\n",
    "depths_ada = [1, 2, 10]\n",
    "combin = list(itertools.product(depths_ada, tree_counts4))\n",
    "com_score = dict()\n",
    "for dep, num in combin: \n",
    "    adaboost1 = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=dep), n_estimators=num, learning_rate=0.05)\n",
    "    ada_scr = cross_val_score(adaboost1, x_train, y_train)\n",
    "    com_score[str(dep) + ',' + str(num)] = np.mean(ada_scr)\n",
    "com_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50960000000000005"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the best combination of depth and number of trees that we found was 2 and 256\n",
    "adaboost_optimal = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=2), n_estimators=256, learning_rate=0.05)\n",
    "adaboost_optimal.fit(x_train, y_train)\n",
    "ada_pred = adaboost.predict(x_test)\n",
    "optimal_ada = adaboost.score(x_train, ada_pred)\n",
    "optimal_ada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similar to Question 3.4, the optimal number of trees for AdaBoost is the highest number tested (256)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (3pt): Meta-classifier\n",
    "\n",
    "We have so far explored techniques that grow a collection of trees either by creating multiple copies of the original training set, or through a sequential procedure, and then combines these trees into a single classifier. Consider an alternate scenario where you are provided with a pre-trained collection of trees, say from different participants of a data science competition for Higgs boson discovery. What would be a good strategy to combine these pre-fitted trees into a single powerful classifier? Of course, a simple approach would be to take the majority vote from the individual trees. Can we do better than this simple combination strategy?\n",
    "\n",
    "A collection of 100 decision tree classifiers is provided in the file `models.npy` and can be loaded into an array by executing:\n",
    "\n",
    "`models = np.load('models.npy')`\n",
    "\n",
    "You can make predictions using the $i^\\text{th}$ model on an array of predictors `x` by executing:\n",
    "\n",
    "`model[i].predict(x)`  &nbsp;&nbsp;&nbsp;\n",
    "or &nbsp;&nbsp;&nbsp;\n",
    "`model[i].predict_proba(x)`\n",
    "\n",
    "and score the model on predictors `x` and labels `y` by using:\n",
    "\n",
    "`model[i].score(x, y)`.\n",
    "\n",
    "1. Implement a strategy to combine the provided decision tree classifiers, and compare the test perfomance of your approach with the majority vote classifier. Explain your strategy/algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our strategy is to use each of the models as a predictor and build a logistic regression model with cross-validation that uses the predictions of the other models to make a final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.load('models.npy', encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function takes x's and a list of models and returns a dict with the number of the model as the key and the list of \n",
    "#predictions as the value\n",
    "def model_dict(x, model_list):\n",
    "    models_dict = dict()\n",
    "    for m in range(len(model_list)):\n",
    "        predictions = model_list[m].predict(x)\n",
    "        models_dict[m] = predictions\n",
    "    return models_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9  ...    90   91   92   93  \\\n",
       "0  1.0  1.0  0.0  1.0  1.0  1.0  0.0  1.0  1.0  1.0 ...   1.0  1.0  1.0  1.0   \n",
       "1  1.0  0.0  1.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0 ...   0.0  0.0  1.0  1.0   \n",
       "2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  1.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
       "3  0.0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  1.0 ...   0.0  1.0  1.0  1.0   \n",
       "4  0.0  0.0  0.0  0.0  1.0  0.0  1.0  0.0  1.0  1.0 ...   0.0  0.0  0.0  0.0   \n",
       "\n",
       "    94   95   96   97   98   99  \n",
       "0  1.0  1.0  1.0  1.0  1.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "3  0.0  1.0  1.0  0.0  0.0  1.0  \n",
       "4  0.0  0.0  1.0  0.0  1.0  0.0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict = model_dict(x_train, models)\n",
    "test_dict = model_dict(x_test, models)\n",
    "mx_train= pd.DataFrame.from_dict(train_dict)\n",
    "mx_test = pd.DataFrame.from_dict(test_dict)\n",
    "mx_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68959999999999999"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_classifier = LogisticRegressionCV(cv = 5, penalty = 'l2')\n",
    "meta_classifier.fit(mx_train, y_train)\n",
    "meta_classifier.score(mx_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "---\n",
    "\n",
    "## APCOMP209a - Homework Question\n",
    "â€‹\n",
    "We've worked with imputation methods on missing data in Homework 6.  We've worked with Decision Trees in HW7 and here.  Now let's see what happens if we try to work with Decision Trees and Missing Data at the same time! We'll be working with a dataset from the UCI Machine Learning Repository that uses a variety of wine chemical predictors to classify wines grown in the same region in Italy.  Each line represents 13 (mostly chemical) predictors of the response variable wine class, including things like alcohol content, hue , and phenols.  Unfortunately some of the predictor values were lost in measurement. Please load `wine_quality_missing.csv`. \n",
    "â€‹\n",
    "*Note*: As in HW6 be careful of reading/treating column names and row names in this data set.\n",
    "â€‹\n",
    "â€‹\n",
    "1. Remove all observations that contain and missing values, split the dataset into a 75-25 train-test split, and fit the sklearn DecisionTreeClassifier and RandomForestClassifier.   Use cross-validation to find the optimal tree depth for each method.  Report the optimal tree-depth, overall classification rate and confusion matrix on the test set for each method.\n",
    "2. Restart with a fresh copy of the data and impute the missing data via mean imputation.  Split the data 75-25 and again fit DecisionTreeClassifier and RandomForestClassifier using cross-validation to find the optimal tree depth.  Report the optimal tree depth, overall classification rate and confusion matrix on the test set for each method.  \n",
    "3. Again restart with a fresh copy of the data but this time let's try something different.  As discussed in section, CART Decision Trees can take advantage of surrogate splits to handle missing data.  Split the data 75-25 and construct a **custom** decision tree model and train it on the training set with missing data. Report the optimal tree depth, overall classification rate and confusion matrix on the test set and compare your results to the Imputation and DecisionTree model results in part 1 & 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

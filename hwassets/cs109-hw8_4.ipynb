{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 8\n",
    "# Ensemble methods\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as random\n",
    "import itertools as itertools\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Boson Discovery\n",
    "\n",
    "The discovery of the Higgs boson in July 2012 marked a fundamental breakthrough in particle physics. The Higgs boson particle was discovered through experiments at the Large Hadron Collider at CERN, by colliding beams of protons at high energy. A key challenge in analyzing the results of these experiments is to differentiate between a collision that produces Higgs bosons and collisions thats produce only background noise. We shall explore the use of ensemble methods for this classification task.\n",
    "\n",
    "You are provided with data from Monte-Carlo simulations of collisions of particles in a particle collider experiment. The training set is available in `Higgs_train.csv` and the test set is in `Higgs_test.csv`. Each row in these files corresponds to a particle colision described by 28 features (columns 1-28), of which the first 21 features are kinematic properties measured by the particle detectors in the accelerator, and the remaining features are derived by physicists from the the first 21 features. The class label is provided in the last column, with a label of 1 indicating that the collision produces Higgs bosons (signal), and a label of 0 indicating that the collision produces other particles (background). \n",
    "\n",
    "The data set provided to you is a small subset of the HIGGS data set in the UCI machine learning repository. The following paper contains further details about the data set and the predictors used: <a href = \"https://www.nature.com/articles/ncomms5308\">Baldi et al., Nature Communications 5, 2014</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton pT</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>jet 2 pt</th>\n",
       "      <th>...</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.376816</td>\n",
       "      <td>-1.583727</td>\n",
       "      <td>-1.707552</td>\n",
       "      <td>0.990897</td>\n",
       "      <td>0.114397</td>\n",
       "      <td>1.253553</td>\n",
       "      <td>0.619859</td>\n",
       "      <td>-1.479572</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>0.753658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522449</td>\n",
       "      <td>1.318622</td>\n",
       "      <td>0.982398</td>\n",
       "      <td>1.359610</td>\n",
       "      <td>0.964809</td>\n",
       "      <td>1.309991</td>\n",
       "      <td>1.083203</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.707330</td>\n",
       "      <td>0.087603</td>\n",
       "      <td>-0.399742</td>\n",
       "      <td>0.918742</td>\n",
       "      <td>-1.229936</td>\n",
       "      <td>1.172847</td>\n",
       "      <td>-0.552574</td>\n",
       "      <td>0.886053</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>1.298317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.439696</td>\n",
       "      <td>0.828885</td>\n",
       "      <td>0.992241</td>\n",
       "      <td>1.157820</td>\n",
       "      <td>2.215780</td>\n",
       "      <td>1.189586</td>\n",
       "      <td>0.937976</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.617290</td>\n",
       "      <td>0.265839</td>\n",
       "      <td>-1.345227</td>\n",
       "      <td>1.154581</td>\n",
       "      <td>1.036646</td>\n",
       "      <td>0.954822</td>\n",
       "      <td>0.377252</td>\n",
       "      <td>-0.147960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.063507</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.542413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.024506</td>\n",
       "      <td>1.026255</td>\n",
       "      <td>0.986289</td>\n",
       "      <td>0.927720</td>\n",
       "      <td>1.371080</td>\n",
       "      <td>0.981672</td>\n",
       "      <td>0.917436</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.850992</td>\n",
       "      <td>-0.380876</td>\n",
       "      <td>-0.071264</td>\n",
       "      <td>1.468704</td>\n",
       "      <td>-0.795133</td>\n",
       "      <td>0.691818</td>\n",
       "      <td>0.883260</td>\n",
       "      <td>0.496881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.616349</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.520171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.197755</td>\n",
       "      <td>1.100534</td>\n",
       "      <td>0.987262</td>\n",
       "      <td>1.353453</td>\n",
       "      <td>1.455383</td>\n",
       "      <td>0.994682</td>\n",
       "      <td>0.953553</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.767540</td>\n",
       "      <td>-0.691572</td>\n",
       "      <td>-0.040191</td>\n",
       "      <td>0.614843</td>\n",
       "      <td>0.143765</td>\n",
       "      <td>0.748614</td>\n",
       "      <td>0.397057</td>\n",
       "      <td>-0.873640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.147862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502034</td>\n",
       "      <td>1.550981</td>\n",
       "      <td>0.921948</td>\n",
       "      <td>0.864080</td>\n",
       "      <td>0.982839</td>\n",
       "      <td>1.373222</td>\n",
       "      <td>0.601492</td>\n",
       "      <td>0.918621</td>\n",
       "      <td>0.957063</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lepton pT   lepton eta   lepton phi   missing energy magnitude  \\\n",
       "0   0.376816    -1.583727    -1.707552                   0.990897   \n",
       "1   0.707330     0.087603    -0.399742                   0.918742   \n",
       "2   0.617290     0.265839    -1.345227                   1.154581   \n",
       "3   0.850992    -0.380876    -0.071264                   1.468704   \n",
       "4   0.767540    -0.691572    -0.040191                   0.614843   \n",
       "\n",
       "    missing energy phi   jet 1 pt   jet 1 eta   jet 1 phi   jet 1 b-tag  \\\n",
       "0             0.114397   1.253553    0.619859   -1.479572      2.173076   \n",
       "1            -1.229936   1.172847   -0.552574    0.886053      2.173076   \n",
       "2             1.036646   0.954822    0.377252   -0.147960      0.000000   \n",
       "3            -0.795133   0.691818    0.883260    0.496881      0.000000   \n",
       "4             0.143765   0.748614    0.397057   -0.873640      0.000000   \n",
       "\n",
       "    jet 2 pt   ...     jet 4 phi   jet 4 b-tag      m_jj     m_jjj      m_lv  \\\n",
       "0   0.753658   ...      0.397156      0.000000  0.522449  1.318622  0.982398   \n",
       "1   1.298317   ...      0.236231      0.000000  0.439696  0.828885  0.992241   \n",
       "2   1.063507   ...     -0.542413      0.000000  1.024506  1.026255  0.986289   \n",
       "3   1.616349   ...     -1.520171      0.000000  1.197755  1.100534  0.987262   \n",
       "4   1.147862   ...      0.502034      1.550981  0.921948  0.864080  0.982839   \n",
       "\n",
       "      m_jlv      m_bb     m_wbb    m_wwbb   class  \n",
       "0  1.359610  0.964809  1.309991  1.083203     1.0  \n",
       "1  1.157820  2.215780  1.189586  0.937976     1.0  \n",
       "2  0.927720  1.371080  0.981672  0.917436     1.0  \n",
       "3  1.353453  1.455383  0.994682  0.953553     1.0  \n",
       "4  1.373222  0.601492  0.918621  0.957063     0.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('Higgs_train.csv')\n",
    "df_test = pd.read_csv('Higgs_test.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting into x and y training and testing\n",
    "x_train = df_train.loc[:, df_train.columns != ' class']\n",
    "y_train = df_train[\" class\"]\n",
    "x_test = df_test.loc[:, df_test.columns != ' class']\n",
    "y_test = df_test[' class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (2pt): Single Decision Tree\n",
    "We start by building a basic model which we will use as our base model for comparison. \n",
    "\n",
    "1. Fit a decision tree model to the training set and report the classification accuracy of the model on the test set. Use 5-fold cross-validation to choose the (maximum) depth for the tree. You will use the max_depth you find here throughout the homework. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10': 0.60780235355533108,\n",
       " '2': 0.62200179832124691,\n",
       " '3': 0.62199795985540496,\n",
       " '4': 0.63620340222419902,\n",
       " '5': 0.63180308094460802,\n",
       " '6': 0.62540651752307641,\n",
       " '7': 0.62060147800127252,\n",
       " '8': 0.61520051665750231,\n",
       " '9': 0.6146038751231907}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decision tree with CV to determine best depth (found to be 4)\n",
    "depths = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "tree_scores = dict()\n",
    "for i, d in enumerate(depths):\n",
    "    tree_model =  DecisionTreeClassifier(max_depth = d, random_state = 1)\n",
    "    scores = cross_val_score(tree_model, x_train, y_train)\n",
    "    tree_scores[str(d)] = np.mean(scores)\n",
    "tree_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64959999999999996"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_model =  DecisionTreeClassifier(max_depth = 4, random_state = 1)\n",
    "tree_model.fit(x_train, y_train)\n",
    "tree_model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (15pt): Dropout-based Approach\n",
    "We start with a simple method inspired from the idea of 'dropout' in machine learning, where we fit multiple decision trees on random subsets of predictors, and combine them through a majority vote. The procedure is described below.\n",
    "\n",
    "- For each predictor in the training sample, set the predictor values to 0 with probability $p$  (i.e. drop the predictor by setting it to 0). Repeat this for $B$ trials to create $B$ separate training sets.\n",
    "\n",
    "\n",
    "- Fit decision tree models $\\hat{h}^1(x), \\ldots, \\hat{h}^B(x) \\in \\{0,1\\}$ to the $B$ training sets. \n",
    "\n",
    "- Combine the decision tree models into a single classifier by taking a majority vote:\n",
    "$$\n",
    "\\hat{H}_{maj}(x) \\,=\\, majority\\Big(\\hat{h}^1(x), \\ldots, \\hat{h}^B(x)\\Big).\n",
    "$$\n",
    "\n",
    "\n",
    "We shall refer to the combined classifier as an ** *ensemble classifier* **. Implement the described dropout approach, and answer the following questions:\n",
    "1. Apply the dropout procedure with $p = 0.5$ for different number of trees (say $2, 4, 8, 16, \\ldots, 256$), and evaluate the training and test accuracy of the combined classifier. Does an increase in the number of trees improve the training and test performance? Explain your observations in terms of the bias-variance trade-off for the classifier.\n",
    "- Fix the number of trees to 64 and apply the dropout procedure with different dropout rates $p = 0.1, 0.3, 0.5, 0.7, 0.9$. Based on your results, explain how the dropout rate influences the bias and variance of the combined classifier.\n",
    "- Apply 5-fold cross-validation to choose the optimal combination of the dropout rate and number of trees. How does the test performance of an ensemble of trees fitted with the optimal dropout rate and number of trees compare with the single decision tree model in Question 1?\n",
    "[hint: Training with large number of trees can take long time. You may need to restrict the max number of trees.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flip(probability):\n",
    "    return 0 if random.random() < probability else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function creates b new sets with probability p to determine the new columns\n",
    "def create_sets(train, test, p, b):\n",
    "    set_list = []\n",
    "    for i in range(b):\n",
    "        new_columns = []\n",
    "        while len(new_columns) == 0:\n",
    "            for c in train.columns:\n",
    "                if flip(p) == 1:\n",
    "                    new_columns.append(c)\n",
    "        set_list.append((train[new_columns], test[new_columns]))\n",
    "    return set_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_array(train_list, train_y):\n",
    "    m_array = []\n",
    "    count = 0\n",
    "    for train in train_list:\n",
    "        tree_model = DecisionTreeClassifier(max_depth = 4, random_state = 1)\n",
    "        tree_model.fit(train, train_y)\n",
    "        m_array.append(pickle.dumps(tree_model))\n",
    "    return m_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensemble_classifier(m_array, x_list):\n",
    "    ps = []\n",
    "    predictions = []\n",
    "    for m, test in zip(m_array, x_list):\n",
    "        m1 = pickle.loads(m)\n",
    "        p1 = m1.predict(test)\n",
    "        ps.append(p1)\n",
    "    for i in range(len(ps[0])):\n",
    "        voting = []\n",
    "        for p in ps:\n",
    "            voting.append(p[i])\n",
    "        if np.mean(voting) >= 0.5:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'128': (0.68320000000000003, 0.66359999999999997),\n",
       " '16': (0.68079999999999996, 0.65680000000000005),\n",
       " '2': (0.60919999999999996, 0.60199999999999998),\n",
       " '256': (0.68320000000000003, 0.66200000000000003),\n",
       " '32': (0.67579999999999996, 0.65300000000000002),\n",
       " '4': (0.66120000000000001, 0.64880000000000004),\n",
       " '64': (0.67479999999999996, 0.65720000000000001),\n",
       " '8': (0.65839999999999999, 0.63219999999999998)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_counts = [2**x for x in range(1, 9)]\n",
    "c_scores = dict()\n",
    "for count in tree_counts:\n",
    "    set_list = create_sets(x_train, x_test, 0.5, count)\n",
    "    train_list = [x[0] for x in set_list]\n",
    "    test_list = [x[1] for x in set_list]\n",
    "    m = model_array(train_list, y_train)\n",
    "    train_predictions = ensemble_classifier(m, train_list)\n",
    "    test_predictions = ensemble_classifier(m, test_list)\n",
    "    c_scores[str(count)] = (metrics.accuracy_score(y_train, train_predictions), metrics.accuracy_score(y_test, test_predictions)) \n",
    "c_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.1', (0.66020000000000001, 0.64959999999999996)),\n",
       " ('0.3', (0.66300000000000003, 0.65039999999999998)),\n",
       " ('0.5', (0.67859999999999998, 0.66139999999999999)),\n",
       " ('0.7', (0.69299999999999995, 0.66520000000000001)),\n",
       " ('0.9', (0.60340000000000005, 0.5776))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "p_scores = dict()\n",
    "for prob in probabilities:\n",
    "    set_list = create_sets(x_train, x_test, prob, 64)\n",
    "    train_list = [x[0] for x in set_list]\n",
    "    test_list = [x[1] for x in set_list]\n",
    "    m = model_array(train_list, y_train)\n",
    "    train_predictions = ensemble_classifier(m, train_list)\n",
    "    test_predictions = ensemble_classifier(m, test_list)\n",
    "    p_scores[str(prob)] = (metrics.accuracy_score(y_train, train_predictions), metrics.accuracy_score(y_test, test_predictions)) \n",
    "p_scores = sorted(p_scores.items())\n",
    "p_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.1,16': 0.6399999999999999,\n",
       " '0.1,2': 0.60919999999999996,\n",
       " '0.1,32': 0.6399999999999999,\n",
       " '0.1,4': 0.62619999999999998,\n",
       " '0.1,64': 0.6399999999999999,\n",
       " '0.1,8': 0.63979999999999992,\n",
       " '0.3,16': 0.64079999999999993,\n",
       " '0.3,2': 0.623,\n",
       " '0.3,32': 0.64540000000000008,\n",
       " '0.3,4': 0.63559999999999994,\n",
       " '0.3,64': 0.64680000000000004,\n",
       " '0.3,8': 0.63760000000000006,\n",
       " '0.5,16': 0.64380000000000004,\n",
       " '0.5,2': 0.60799999999999998,\n",
       " '0.5,32': 0.64980000000000004,\n",
       " '0.5,4': 0.64160000000000006,\n",
       " '0.5,64': 0.65380000000000005,\n",
       " '0.5,8': 0.63619999999999999,\n",
       " '0.7,16': 0.62200000000000011,\n",
       " '0.7,2': 0.57319999999999993,\n",
       " '0.7,32': 0.65500000000000003,\n",
       " '0.7,4': 0.62039999999999995,\n",
       " '0.7,64': 0.64839999999999998,\n",
       " '0.7,8': 0.63619999999999999,\n",
       " '0.9,16': 0.55879999999999996,\n",
       " '0.9,2': 0.52560000000000007,\n",
       " '0.9,32': 0.57199999999999995,\n",
       " '0.9,4': 0.58040000000000003,\n",
       " '0.9,64': 0.55420000000000003,\n",
       " '0.9,8': 0.57040000000000002}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CV to determine best p and b combo\n",
    "#tree_counts1 only goes to 64 trees to avoid lenghty computation times\n",
    "tree_counts1 = [2**x for x in range(1, 7)]\n",
    "combinations = list(itertools.product(probabilities, tree_counts1))\n",
    "combo_scores = dict()\n",
    "for p, b in combinations:\n",
    "    scores = []\n",
    "    train_sets = [x[0] for x in create_sets(x_train, x_test, p, b)]\n",
    "    for train, valid in KFold(5, shuffle=False).split(range(5000)):\n",
    "        train_list1 = []\n",
    "        test_list1 = []\n",
    "        ny_train = y_train[train]\n",
    "        ny_test = y_train[valid]\n",
    "        for s in train_sets:\n",
    "            nx_train = s.iloc[train]\n",
    "            nx_test = s.iloc[valid]\n",
    "            train_list1.append(nx_train)\n",
    "            test_list1.append(nx_test)\n",
    "        m = model_array(train_list1, ny_train)\n",
    "        test_predictions = ensemble_classifier(m, test_list1)\n",
    "        scores.append(metrics.accuracy_score(ny_test, test_predictions))\n",
    "    combo_scores[str(p) + ',' + str(b)] = np.mean(scores)\n",
    "combo_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65820000000000001"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the best combination found by our model is p = 0.5 and b = 64\n",
    "set_list = create_sets(x_train, x_test, 0.5, 64)\n",
    "train_list = [x[0] for x in set_list]\n",
    "test_list = [x[1] for x in set_list]\n",
    "m = model_array(train_list, y_train)\n",
    "test_predictions = ensemble_classifier(m, test_list)\n",
    "metrics.accuracy_score(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (15pt): Random Forests\n",
    "\n",
    "We now move to a more sophisticated ensemble technique, namely random forest:\n",
    "1. How does a random forest approach differ from the dropout procedure described in Question 2? \n",
    " \n",
    "- Fit random forest models to the training set for different number of trees (say $2, 4, 8, 16, \\ldots, 256$), and evaluate the training and test accuracies of the models. You may set the number of predictors for each tree in the random forest model to $\\sqrt{p}$, where $p$ is the total number of predictors. \n",
    "\n",
    "- Based on your results, do you find that a larger number of trees necessarily improves the test accuracy of a random forest model? Explain how the number of trees effects the training and test accuracy of a random forest classifier, and how this relates to the bias-variance trade-off for the classifier. \n",
    "  \n",
    "- Fixing the number of trees to a reasonable value, apply 5-fold cross-validation to choose the optimal value for the  number of predictors. How does the test performance of random forest model fitted with the optimal number of trees compare with the dropout approach in Question 2?  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does a random forest approach differ from the dropout procedure described in Question 2?\n",
    "- The random forest approach is much different, because rather than dropping certain predictors, different decision trees are built using different samples that are obtained through bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'128': 0.70279999999999998,\n",
       " '16': 0.66800000000000004,\n",
       " '2': 0.58179999999999998,\n",
       " '256': 0.70299999999999996,\n",
       " '32': 0.68300000000000005,\n",
       " '4': 0.61319999999999997,\n",
       " '64': 0.69199999999999995,\n",
       " '8': 0.64239999999999997}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the square root of 28 is about 5, so we use 5 predictors for each tree\n",
    "tree_counts2 = [2**x for x in range(1, 9)]\n",
    "train_scores = dict()\n",
    "test_scores = dict()\n",
    "for c in tree_counts2:\n",
    "    rand_forest = RandomForestClassifier(n_estimators = c, max_features = 5, random_state = 1)\n",
    "    rand_forest.fit(x_train, y_train)\n",
    "    tr_pred = rand_forest.predict(x_train)\n",
    "    tst_pred = rand_forest.predict(x_test)\n",
    "    train_scores[str(c)] = metrics.accuracy_score(y_train, tr_pred)\n",
    "    test_scores[str(c)] = metrics.accuracy_score(y_test, tst_pred)\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x104ff3a58>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111e8ac88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot of test scores vs. number of trees\n",
    "x1, y1 = zip(*test_scores.items())\n",
    "plt.plot(x1, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your results, do you find that a larger number of trees necessarily improves the test accuracy of a random forest model? Explain how the number of trees effects the training and test accuracy of a random forest classifier, and how this relates to the bias-variance trade-off for the classifier.\n",
    "- Our results show that a larger number of trees does necessarily improve the test accuracy of a random forest model. TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing the number of trees to a reasonable value, apply 5-fold cross-validation to choose the optimal value for the number of predictors. How does the test performance of random forest model fitted with the optimal number of trees compare with the dropout approach in Question 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 0.64059820570914217,\n",
       " '10': 0.67260141466658607,\n",
       " '11': 0.68320445684268927,\n",
       " '12': 0.67320261437908491,\n",
       " '13': 0.67640173575425377,\n",
       " '14': 0.6777996570330771,\n",
       " '15': 0.66980461249247902,\n",
       " '16': 0.67300205453884188,\n",
       " '17': 0.673401015082292,\n",
       " '18': 0.67060133405880329,\n",
       " '19': 0.67380309437923847,\n",
       " '2': 0.66000021111562124,\n",
       " '20': 0.67179869549738358,\n",
       " '21': 0.67540061588184441,\n",
       " '22': 0.66359925265070052,\n",
       " '23': 0.67080357322785222,\n",
       " '24': 0.67359941578549887,\n",
       " '25': 0.66800077345086706,\n",
       " '26': 0.67020189370712313,\n",
       " '27': 0.66840045370666257,\n",
       " '28': 0.67180157434676513,\n",
       " '3': 0.66439933287463659,\n",
       " '4': 0.67520125556217681,\n",
       " '5': 0.67699789748033512,\n",
       " '6': 0.67319613696797675,\n",
       " '7': 0.6806029366182923,\n",
       " '8': 0.67740285562666314,\n",
       " '9': 0.67179893540149882}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fixing the number of trees to 64, because it's accuracy is very close to that of a forest with 128 trees, but will be less expensive\n",
    "feature_count = np.arange(1, 29)\n",
    "fscores = dict()\n",
    "for f in feature_count:\n",
    "    rand_forest = RandomForestClassifier(n_estimators = 64, max_features = f, random_state = 1)\n",
    "    scores = cross_val_score(rand_forest, x_train, y_train)\n",
    "    fscores[str(f)] = np.mean(scores)\n",
    "fscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70140000000000002"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the best number of features we found was 11 and the best practical number of trees we found was 128\n",
    "rand_forest = RandomForestClassifier(n_estimators = 128, max_features = 11, random_state = 1)\n",
    "rand_forest.fit(x_train, y_train)\n",
    "tst_pred = rand_forest.predict(x_test)\n",
    "optimal_rand_forest_score = metrics.accuracy_score(y_test, tst_pred)\n",
    "optimal_rand_forest_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (15pt): Boosting\n",
    "\n",
    "We next compare the random forest model with the approach of boosting:\n",
    "\n",
    "\n",
    "1. Apply the AdaBoost algorithm to fit an ensemble of decision trees. Set the learning rate to 0.05, and try out different tree depths for the base learners: 1, 2, 10, and unrestricted depth.  Make a plot of the training accuracy of the ensemble classifier as a function of tree depths. Make a similar plot of the test accuracies as a function of number of trees (say $2, 4, 8, 16, \\ldots, 256$).\n",
    "- How does the number of trees influence the training and test performance? Compare and contrast between the trends you see in the training and test performance of AdaBoost and that of the random forest models in Question 3. Give an explanation for your observations.\n",
    "- How does the tree depth of the base learner impact the training and test performance? Recall that with random forests, we allow the depth of the individual trees to be unrestricted. Would you recommend the same strategy for boosting? Explain your answer.\n",
    "- Apply 5-fold cross-validation to choose the optimal number of trees $B$ for the ensemble and the optimal tree depth for the base learners. How does an ensemble classifier fitted with the optimal number of trees and the optimal tree depth compare with the random forest model fitted in Question 3.4? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score as a function of depth {'1': 0.68079999999999996, '2': 0.74019999999999997, '10': 1.0}\n",
      "\n",
      "Train Score as a function of number of trees {'2': 0.92520000000000002, '4': 0.97719999999999996, '8': 0.99180000000000001, '16': 0.99939999999999996, '32': 0.99980000000000002, '64': 1.0, '128': 1.0, '256': 1.0}\n",
      "\n",
      "Test Score as a function of depth {'1': 0.68079999999999996, '2': 0.74019999999999997, '10': 1.0}\n",
      "\n",
      "Test Score as a function of number of trees {'2': 0.62739999999999996, '4': 0.61480000000000001, '8': 0.5978, '16': 0.59699999999999998, '32': 0.61199999999999999, '64': 0.63480000000000003, '128': 0.64439999999999997, '256': 0.6542}\n"
     ]
    }
   ],
   "source": [
    "# is n_estimators = 100 good? I can't put none in depths_ada because I can't plot... how do I show unrestricted depth?\n",
    "boost_train_score_dep = dict()\n",
    "boost_train_score_num = dict()\n",
    "boost_test_score_dep = dict()\n",
    "boost_test_score_num = dict()\n",
    "depths_ada = [1, 2, 10]\n",
    "tree_counts3 = [2**x for x in range(1,9)]\n",
    "for i, d in enumerate(depths_ada): \n",
    "    for c in tree_counts3: \n",
    "        adaboost = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=d), n_estimators=c, learning_rate=0.05)\n",
    "        adaboost.fit(x_train, y_train)\n",
    "        sc_train = adaboost.score(x_train, y_train)\n",
    "        sc_test = adaboost.score(x_test, y_test)\n",
    "        boost_train_score_dep[str(d)] = sc_train\n",
    "        boost_train_score_num[str(c)] = sc_train\n",
    "        boost_test_score_dep[str(d)] = sc_train\n",
    "        boost_test_score_num[str(c)] = sc_test\n",
    "print(\"Train Score as a function of depth\", boost_train_score_dep)\n",
    "print(\"\")\n",
    "print(\"Train Score as a function of number of trees\", boost_train_score_num)\n",
    "print(\"\")\n",
    "print(\"Test Score as a function of depth\", boost_test_score_dep)\n",
    "print(\"\")\n",
    "print(\"Test Score as a function of number of trees\", boost_test_score_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113418b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x2, y2 = zip(*boost_train_score_dep.items())\n",
    "plt.plot(x2, y2)\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.xlabel('Tree Depths');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11192e668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x3, y3 = zip(*boost_test_score_num.items())\n",
    "plt.plot(x3, y3)\n",
    "plt.ylabel('Testing Accuracy')\n",
    "plt.xlabel('Number of Trees');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the number of trees influence the training and test performance? \n",
    "<tr>\n",
    "As the number of trees increases, the training performance increases in accuracy. It starts 0.912 accuracy with 2 trees and reaches 1 when number of trees are greater than 64. The testing performance follows a similar increasing trend, but starts with 0.62 accuracy with 2 trees and reaches 0.6588 accuracy with 256 trees. A similar trend is found in the results on question 3, yet, for example, the testing accuracy is able to reach 0.7 accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the tree depth of the base learner impact the training and test performance? \n",
    "<tr>\n",
    "As the depth of the base learner increases, the training accuracy increases steadily from 0.68 when depth is set to 1 to 1 when depth is set to 10. The testing accuracy instead increases at a similar rate, starting at 0.68 with the depth set at 1 to reaching 1 when depth is set to 10 as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply 5-fold cross-validation to choose the optimal number of trees  BB  for the ensemble and the optimal tree depth for the base learners. How does an ensemble classifier fitted with the optimal number of trees and the optimal tree depth compare with the random forest model fitted in Question 3.4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1,128': 0.65020180734164168,\n",
       " '1,16': 0.59920347035696775,\n",
       " '1,2': 0.60020387051703172,\n",
       " '1,256': 0.66280085175557035,\n",
       " '1,32': 0.61520003684927216,\n",
       " '1,4': 0.60020387051703172,\n",
       " '1,64': 0.64160244433504809,\n",
       " '1,8': 0.60020387051703172,\n",
       " '10,128': 0.63600044334280481,\n",
       " '10,16': 0.593197950643087,\n",
       " '10,2': 0.62600076001623683,\n",
       " '10,256': 0.63920244356735501,\n",
       " '10,32': 0.60579915419405184,\n",
       " '10,4': 0.60560339243611117,\n",
       " '10,64': 0.61000035505809036,\n",
       " '10,8': 0.58460122658175984,\n",
       " '2,128': 0.67480061568992111,\n",
       " '2,16': 0.64300348436736809,\n",
       " '2,2': 0.62100139816118294,\n",
       " '2,256': 0.67700077632971656,\n",
       " '2,32': 0.64640076654162859,\n",
       " '2,4': 0.62100139816118294,\n",
       " '2,64': 0.66139981172325057,\n",
       " '2,8': 0.6254031588654646}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_counts4 = [2**x for x in range(1,9)]\n",
    "depths_ada = [1, 2, 10]\n",
    "combin = list(itertools.product(depths_ada, tree_counts4))\n",
    "com_score = dict()\n",
    "for dep, num in combin: \n",
    "    adaboost1 = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=dep), n_estimators=num, learning_rate=0.05)\n",
    "    ada_scr = cross_val_score(adaboost1, x_train, y_train)\n",
    "    com_score[str(dep) + ',' + str(num)] = np.mean(ada_scr)\n",
    "com_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67700077632971656"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(com_score.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51039999999999996"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the best combination of depth and number of trees that we found was 2 and 256\n",
    "adaboost_optimal = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=2), n_estimators=256, learning_rate=0.05)\n",
    "adaboost_optimal.fit(x_train, y_train)\n",
    "ada_pred = adaboost.predict(x_test)\n",
    "optimal_ada = adaboost.score(x_train, ada_pred)\n",
    "optimal_ada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (3pt): Meta-classifier\n",
    "\n",
    "We have so far explored techniques that grow a collection of trees either by creating multiple copies of the original training set, or through a sequential procedure, and then combines these trees into a single classifier. Consider an alternate scenario where you are provided with a pre-trained collection of trees, say from different participants of a data science competition for Higgs boson discovery. What would be a good strategy to combine these pre-fitted trees into a single powerful classifier? Of course, a simple approach would be to take the majority vote from the individual trees. Can we do better than this simple combination strategy?\n",
    "\n",
    "A collection of 100 decision tree classifiers is provided in the file `models.npy` and can be loaded into an array by executing:\n",
    "\n",
    "`models = np.load('models.npy')`\n",
    "\n",
    "You can make predictions using the $i^\\text{th}$ model on an array of predictors `x` by executing:\n",
    "\n",
    "`model[i].predict(x)`  &nbsp;&nbsp;&nbsp;\n",
    "or &nbsp;&nbsp;&nbsp;\n",
    "`model[i].predict_proba(x)`\n",
    "\n",
    "and score the model on predictors `x` and labels `y` by using:\n",
    "\n",
    "`model[i].score(x, y)`.\n",
    "\n",
    "1. Implement a strategy to combine the provided decision tree classifiers, and compare the test perfomance of your approach with the majority vote classifier. Explain your strategy/algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')], dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = np.load('models.npy', encoding = 'latin1')\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "---\n",
    "\n",
    "## APCOMP209a - Homework Question\n",
    "â€‹\n",
    "We've worked with imputation methods on missing data in Homework 6.  We've worked with Decision Trees in HW7 and here.  Now let's see what happens if we try to work with Decision Trees and Missing Data at the same time! We'll be working with a dataset from the UCI Machine Learning Repository that uses a variety of wine chemical predictors to classify wines grown in the same region in Italy.  Each line represents 13 (mostly chemical) predictors of the response variable wine class, including things like alcohol content, hue , and phenols.  Unfortunately some of the predictor values were lost in measurement. Please load `wine_quality_missing.csv`. \n",
    "â€‹\n",
    "*Note*: As in HW6 be careful of reading/treating column names and row names in this data set.\n",
    "â€‹\n",
    "â€‹\n",
    "1. Remove all observations that contain and missing values, split the dataset into a 75-25 train-test split, and fit the sklearn DecisionTreeClassifier and RandomForestClassifier.   Use cross-validation to find the optimal tree depth for each method.  Report the optimal tree-depth, overall classification rate and confusion matrix on the test set for each method.\n",
    "2. Restart with a fresh copy of the data and impute the missing data via mean imputation.  Split the data 75-25 and again fit DecisionTreeClassifier and RandomForestClassifier using cross-validation to find the optimal tree depth.  Report the optimal tree depth, overall classification rate and confusion matrix on the test set for each method.  \n",
    "3. Again restart with a fresh copy of the data but this time let's try something different.  As discussed in section, CART Decision Trees can take advantage of surrogate splits to handle missing data.  Split the data 75-25 and construct a **custom** decision tree model and train it on the training set with missing data. Report the optimal tree depth, overall classification rate and confusion matrix on the test set and compare your results to the Imputation and DecisionTree model results in part 1 & 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
